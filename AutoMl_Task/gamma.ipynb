{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install the needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:13:21.073855Z",
     "iopub.status.busy": "2024-11-16T22:13:21.073001Z",
     "iopub.status.idle": "2024-11-16T22:17:02.146366Z",
     "shell.execute_reply": "2024-11-16T22:17:02.145219Z",
     "shell.execute_reply.started": "2024-11-16T22:13:21.073802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-osumxkue/unsloth_449c26e5080f47b78e9667d430b7dae8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-osumxkue/unsloth_449c26e5080f47b78e9667d430b7dae8\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit f26d4e739ed507de7a9088da53d10fd02f58d160\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Collecting unsloth-zoo>=2024.11.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2024.11.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (21.3)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting transformers>=4.46.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.3)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.34.2)\n",
      "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.25.1)\n",
      "Collecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
      "Collecting triton (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2024.11.5-py3-none-any.whl (31 kB)\n",
      "Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.11.7-py3-none-any.whl size=163138 sha256=75f771e4848a9ad07ec558d3a4b8cf7b26c0cd7d843656e2510725c5bf7410af\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kkzt1e33/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth, triton, shtab, hf-transfer, tyro, bitsandbytes, transformers, trl, peft, unsloth-zoo\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "Successfully installed bitsandbytes-0.44.1 hf-transfer-0.1.8 peft-0.13.2 shtab-1.7.1 transformers-4.46.2 triton-3.1.0 trl-0.12.1 tyro-0.8.14 unsloth-2024.11.7 unsloth-zoo-2024.11.5\n",
      "Collecting xformers<0.0.27\n",
      "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting trl<0.9.0\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\n",
      "Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers, trl\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.12.1\n",
      "    Uninstalling trl-0.12.1:\n",
      "      Successfully uninstalled trl-0.12.1\n",
      "Successfully installed trl-0.8.6 xformers-0.0.26.post1\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==2.3.0+cu121\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.18.0+cu121\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.3.0+cu121\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0+cu121) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0+cu121) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0+cu121) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0+cu121) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0+cu121) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0+cu121) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0+cu121)\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0+cu121) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0+cu121) (10.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0+cu121)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0+cu121) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0+cu121) (1.3.0)\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.0\n",
      "    Uninstalling torchvision-0.19.0:\n",
      "      Successfully uninstalled torchvision-0.19.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.4.0\n",
      "    Uninstalling torchaudio-2.4.0:\n",
      "      Successfully uninstalled torchaudio-2.4.0\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n"
     ]
    }
   ],
   "source": [
    "##### %%capture\n",
    "\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 torchaudio==2.3.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:17:46.557681Z",
     "iopub.status.busy": "2024-11-16T22:17:46.557374Z",
     "iopub.status.idle": "2024-11-16T22:18:00.155823Z",
     "shell.execute_reply": "2024-11-16T22:18:00.154318Z",
     "shell.execute_reply.started": "2024-11-16T22:17:46.557647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mlflow pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.157810Z",
     "iopub.status.busy": "2024-11-16T22:18:00.157447Z",
     "iopub.status.idle": "2024-11-16T22:18:00.163123Z",
     "shell.execute_reply": "2024-11-16T22:18:00.161600Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.157772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.167927Z",
     "iopub.status.busy": "2024-11-16T22:18:00.167504Z",
     "iopub.status.idle": "2024-11-16T22:18:00.177794Z",
     "shell.execute_reply": "2024-11-16T22:18:00.176851Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.167881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.179768Z",
     "iopub.status.busy": "2024-11-16T22:18:00.179220Z",
     "iopub.status.idle": "2024-11-16T22:18:00.187611Z",
     "shell.execute_reply": "2024-11-16T22:18:00.186511Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.179725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import TrainerCallback\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "import re\n",
    "from pyngrok import ngrok\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import time\n",
    "from trl import  DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.189316Z",
     "iopub.status.busy": "2024-11-16T22:18:00.188972Z",
     "iopub.status.idle": "2024-11-16T22:18:00.197647Z",
     "shell.execute_reply": "2024-11-16T22:18:00.196459Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.189275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\n",
    "The best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\n",
    "\n",
    "### DESCRIPTION:\n",
    "{}\n",
    "\n",
    "### RESPONSE:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples,EOS_TOKEN):\n",
    "\n",
    "    inputs       = examples[\"series_description\"]\n",
    "\n",
    "    outputs      = examples[\"algorithm\"]\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    for input, output in zip( inputs, outputs):\n",
    "\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "\n",
    "        text = train_prompt.format( input, output) + EOS_TOKEN\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\" : texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.199896Z",
     "iopub.status.busy": "2024-11-16T22:18:00.199125Z",
     "iopub.status.idle": "2024-11-16T22:18:00.526559Z",
     "shell.execute_reply": "2024-11-16T22:18:00.525580Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.199854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c2de2d3b4545bcb52d0ec34c955894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters'],\n",
       "        num_rows: 828\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files=\"/kaggle/input/regression-univariate-train/Regression_Univariate_train.csv\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.528355Z",
     "iopub.status.busy": "2024-11-16T22:18:00.527928Z",
     "iopub.status.idle": "2024-11-16T22:18:00.555380Z",
     "shell.execute_reply": "2024-11-16T22:18:00.554409Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.528308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "\n",
    "# Step 1: Split into 80% train and 20% remaining (val + test)\n",
    "train_valid_test_split = train_data.train_test_split(test_size=0.2, seed=42)\n",
    "train_data = train_valid_test_split['train']\n",
    "remaining_data = train_valid_test_split['test']\n",
    "\n",
    "# Step 2: Split the remaining data into 50% validation and 50% test\n",
    "valid_test_split = remaining_data.train_test_split(test_size=0.5, seed=42)\n",
    "valid_data = valid_test_split['train']\n",
    "test_data = valid_test_split['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:00.557501Z",
     "iopub.status.busy": "2024-11-16T22:18:00.556821Z",
     "iopub.status.idle": "2024-11-16T22:18:29.453894Z",
     "shell.execute_reply": "2024-11-16T22:18:29.453021Z",
     "shell.execute_reply.started": "2024-11-16T22:18:00.557453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Gemma patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c21111e82084163a06bf0d969566804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931727f47e1341de97aa9958bf68948b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a4b7e8fe0e43aaad2b84b64906d72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783d4bb3e9ab4c0db113bd0fc0e16390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f80dc190a0466dba098a53042c5bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151438edee1740d8b5afc0d57224b492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "max_seq_length = 2048# Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2b\",\n",
    "    max_seq_length # Add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "= max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:29.456105Z",
     "iopub.status.busy": "2024-11-16T22:18:29.455394Z",
     "iopub.status.idle": "2024-11-16T22:18:29.462012Z",
     "shell.execute_reply": "2024-11-16T22:18:29.461028Z",
     "shell.execute_reply.started": "2024-11-16T22:18:29.456057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer_config = {\n",
    "            \"Tokenizer\": tokenizer.__class__.__name__,\n",
    "            \"padding_side\": tokenizer.padding_side,\n",
    "            \"add_eos_token\": tokenizer.add_eos_token,\n",
    "            \"pad_token\": tokenizer.pad_token\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:29.463682Z",
     "iopub.status.busy": "2024-11-16T22:18:29.463321Z",
     "iopub.status.idle": "2024-11-16T22:18:33.903984Z",
     "shell.execute_reply": "2024-11-16T22:18:33.902960Z",
     "shell.execute_reply.started": "2024-11-16T22:18:29.463647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "instruction_template=\"DESCRIPTION:\"\n",
    "\n",
    "# Set MLflow's tracking URI and experiment\n",
    "mlflow.set_tracking_uri(\"https://5bb1-156-204-128-49.ngrok-free.app\")\n",
    "mlflow.set_experiment(\"best model gamma-2b\")\n",
    "mlflow.pytorch.autolog(disable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:33.905591Z",
     "iopub.status.busy": "2024-11-16T22:18:33.905239Z",
     "iopub.status.idle": "2024-11-16T22:18:33.914154Z",
     "shell.execute_reply": "2024-11-16T22:18:33.913186Z",
     "shell.execute_reply.started": "2024-11-16T22:18:33.905553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLFlowLoggingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_loss = []\n",
    "        self.eval_loss = []\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        # Initialize logging at the start of training\n",
    "        print(\"Training started.\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Log training and evaluation losses\n",
    "        if 'loss' in logs:\n",
    "            self.training_loss.append(logs['loss'])\n",
    "            mlflow.log_metric(\"training_loss\", logs['loss'], step=state.global_step)\n",
    "\n",
    "        if 'eval_loss' in logs:\n",
    "            self.eval_loss.append(logs['eval_loss'])\n",
    "            mlflow.log_metric(\"validation_loss\", logs['eval_loss'], step=state.global_step)\n",
    "\n",
    "        if 'eval_f1' in logs:\n",
    "            mlflow.log_metric(\"validation_f1\", logs['eval_f1'], step=state.global_step)\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # Log final results at the end of training\n",
    "        print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:33.916007Z",
     "iopub.status.busy": "2024-11-16T22:18:33.915641Z",
     "iopub.status.idle": "2024-11-16T22:18:33.931007Z",
     "shell.execute_reply": "2024-11-16T22:18:33.930179Z",
     "shell.execute_reply.started": "2024-11-16T22:18:33.915963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def configure_and_train_model(\n",
    "    r=128,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    use_gradient_checkpointing=\"none\",\n",
    "    random_state=2048,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    "    train_dataset=None,\n",
    "    valid_dataset=None,\n",
    "    batch_size=2,\n",
    "    grad_accum_steps=16,\n",
    "    warmup_steps=5,\n",
    "    max_steps=-1,\n",
    "    learning_rate=2e-3,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    output_dir=\"outputs\"\n",
    "):\n",
    "    global model\n",
    "    # Load model and tokenizer with specified settings\n",
    "    lora_config = {\n",
    "        \"r\": r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"lora_dropout\": lora_dropout,\n",
    "        \"use_gradient_checkpointing\": use_gradient_checkpointing,\n",
    "        \"random_state\": random_state,\n",
    "        \"use_rslora\": use_rslora,\n",
    "        \"loftq_config\": loftq_config,\n",
    "    }\n",
    "    with mlflow.start_run():\n",
    "      mlflow.set_tag(\"model_name\", \"gamma-2b\")\n",
    "      # Format datasets with EOS token\n",
    "      train_dataset = train_dataset.map(\n",
    "            lambda batch: formatting_prompts_func(batch, EOS_TOKEN=tokenizer.eos_token),\n",
    "            batched=True\n",
    "        )\n",
    "      valid_dataset = valid_dataset.map(\n",
    "            lambda batch: formatting_prompts_func(batch, EOS_TOKEN=tokenizer.eos_token),\n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "      # Configure PEFT model\n",
    "      model = FastLanguageModel.get_peft_model(\n",
    "          model,\n",
    "          r=r,\n",
    "          target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "          lora_alpha=lora_alpha,\n",
    "          lora_dropout=lora_dropout,\n",
    "          bias=\"none\",\n",
    "          use_gradient_checkpointing=use_gradient_checkpointing,\n",
    "          random_state=random_state,\n",
    "          use_rslora=use_rslora,\n",
    "          loftq_config=loftq_config\n",
    "      )\n",
    "    \n",
    "\n",
    "      # Training arguments\n",
    "      training_args = TrainingArguments(\n",
    "          per_device_train_batch_size=batch_size,\n",
    "          gradient_accumulation_steps=grad_accum_steps,\n",
    "          warmup_steps=warmup_steps,\n",
    "          max_steps=max_steps,\n",
    "          learning_rate=learning_rate,\n",
    "          fp16=not is_bfloat16_supported(),\n",
    "          bf16=is_bfloat16_supported(),\n",
    "          logging_steps=1,\n",
    "          optim=\"adamw_8bit\",\n",
    "          weight_decay=weight_decay,\n",
    "          lr_scheduler_type=lr_scheduler_type,\n",
    "          seed=random_state,\n",
    "          output_dir=output_dir,\n",
    "          eval_strategy=\"steps\",\n",
    "          save_strategy=\"epoch\"\n",
    "      )\n",
    "      mlflow.log_params(vars(training_args))\n",
    "\n",
    "      mlflow.log_params(tokenizer_config)\n",
    "      mlflow.log_params(lora_config)\n",
    "      # Trainer setup\n",
    "      trainer = SFTTrainer(\n",
    "          model=model,\n",
    "          tokenizer=tokenizer,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=valid_dataset,\n",
    "          dataset_text_field=\"text\",\n",
    "          max_seq_length=2048,\n",
    "          dataset_num_proc=2,\n",
    "          packing=False,\n",
    "          args=training_args\n",
    "          )\n",
    "      # Initialize the callback\n",
    "      mlflow_callback = MLFlowLoggingCallback()\n",
    "\n",
    "      # Train the model with the callback\n",
    "      trainer.add_callback(mlflow_callback)\n",
    "      # Train the model\n",
    "      trainer.train()\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:33.932372Z",
     "iopub.status.busy": "2024-11-16T22:18:33.932063Z",
     "iopub.status.idle": "2024-11-16T22:18:33.942446Z",
     "shell.execute_reply": "2024-11-16T22:18:33.941564Z",
     "shell.execute_reply.started": "2024-11-16T22:18:33.932340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "2.305 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Show current memory stats\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T22:18:33.943911Z",
     "iopub.status.busy": "2024-11-16T22:18:33.943645Z",
     "iopub.status.idle": "2024-11-16T23:39:34.021314Z",
     "shell.execute_reply": "2024-11-16T23:39:34.020279Z",
     "shell.execute_reply.started": "2024-11-16T22:18:33.943882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e340a89ba34c579883ec4d419e3e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba26f2a5892542c3ae21aa5bfa38d1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.11.7 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e419970e0e84e1c9d6b5694678d066c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8be0888ac3434290829b226c6cc010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 662 | Num Epochs = 8\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 16\n",
      "\\        /    Total batch size = 32 | Total steps = 150\n",
      " \"-____-\"     Number of trainable parameters = 156,893,184\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2024/11/16 22:18:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id b735ae6d403e4f39bbbfc67860b77b58: Failed to log run data: Exception: INVALID_PARAMETER_VALUE: Changing param values is not allowed. Params were already logged='[{'key': 'logging_strategy', 'old_value': 'IntervalStrategy.STEPS', 'new_value': 'steps'}, {'key': 'save_strategy', 'old_value': 'IntervalStrategy.EPOCH', 'new_value': 'epoch'}, {'key': 'accelerator_config', 'old_value': 'AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False)', 'new_value': \"{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\"}, {'key': 'optim', 'old_value': 'OptimizerNames.ADAMW_8BIT', 'new_value': 'adamw_8bit'}, {'key': 'hub_strategy', 'old_value': 'HubStrategy.EVERY_SAVE', 'new_value': 'every_save'}, {'key': 'hub_token', 'old_value': 'None', 'new_value': '<HUB_TOKEN>'}, {'key': 'push_to_hub_token', 'old_value': 'None', 'new_value': '<PUSH_TO_HUB_TOKEN>'}]' for run ID='b735ae6d403e4f39bbbfc67860b77b58'.\n",
      "2024/11/16 22:18:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id b735ae6d403e4f39bbbfc67860b77b58: Failed to log run data: Exception: INVALID_PARAMETER_VALUE: Changing param values is not allowed. Params were already logged='[{'key': 'eval_strategy', 'old_value': 'IntervalStrategy.STEPS', 'new_value': 'steps'}, {'key': 'lr_scheduler_type', 'old_value': 'SchedulerType.LINEAR', 'new_value': 'linear'}]' for run ID='b735ae6d403e4f39bbbfc67860b77b58'.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c958b3799a478ca9588fb02edb804c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113925900003273, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241116_221906-1wtfk1bc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ranahossny52/huggingface/runs/1wtfk1bc' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/ranahossny52/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ranahossny52/huggingface' target=\"_blank\">https://wandb.ai/ranahossny52/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ranahossny52/huggingface/runs/1wtfk1bc' target=\"_blank\">https://wandb.ai/ranahossny52/huggingface/runs/1wtfk1bc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 1:20:04, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.472800</td>\n",
       "      <td>2.485648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.470500</td>\n",
       "      <td>2.388825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.382500</td>\n",
       "      <td>2.171220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.151600</td>\n",
       "      <td>1.922029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.912500</td>\n",
       "      <td>1.614556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.620700</td>\n",
       "      <td>1.273228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.283100</td>\n",
       "      <td>1.013885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.017400</td>\n",
       "      <td>0.914271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.923200</td>\n",
       "      <td>0.856998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>0.815456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.819695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>0.768387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.746063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.732625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.729681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.724641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.751400</td>\n",
       "      <td>0.719430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.726100</td>\n",
       "      <td>0.730582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>0.719433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.710300</td>\n",
       "      <td>0.716428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.166100</td>\n",
       "      <td>0.716105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.727200</td>\n",
       "      <td>0.713933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.698100</td>\n",
       "      <td>0.709693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.728900</td>\n",
       "      <td>0.715757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.715100</td>\n",
       "      <td>0.708200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.710100</td>\n",
       "      <td>0.706621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>0.707550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.704894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.703965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.704222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.695700</td>\n",
       "      <td>0.702540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.740700</td>\n",
       "      <td>0.699151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.698700</td>\n",
       "      <td>0.697037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.697341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.697901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.696833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.714100</td>\n",
       "      <td>0.695272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.693980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.693614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.720500</td>\n",
       "      <td>0.693298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.123300</td>\n",
       "      <td>0.692140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.692832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.691873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.691275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>0.690688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.689440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.689091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.690408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.689292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.688006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.663900</td>\n",
       "      <td>0.688357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.670600</td>\n",
       "      <td>0.688918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>0.688681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.687339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.687114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.686865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.685639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.685172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.684496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.683813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.980700</td>\n",
       "      <td>0.682681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>0.684156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.687477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.669100</td>\n",
       "      <td>0.689633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.694000</td>\n",
       "      <td>0.689570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.686774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.683844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.682431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.663400</td>\n",
       "      <td>0.682402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.684787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.687397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.686908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.681116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>0.681434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>0.681873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.682291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.682909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.683238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.681638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.074200</td>\n",
       "      <td>0.679967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.680728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.682898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.688031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>0.692565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.671400</td>\n",
       "      <td>0.691892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.687042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.667000</td>\n",
       "      <td>0.684062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.682888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.673600</td>\n",
       "      <td>0.684885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.688826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.690210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.686716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.684567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.684574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.667200</td>\n",
       "      <td>0.683931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.684451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.645300</td>\n",
       "      <td>0.684798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.685114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>0.685076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.061300</td>\n",
       "      <td>0.683922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.648900</td>\n",
       "      <td>0.686083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>0.690461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.692952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.636600</td>\n",
       "      <td>0.695786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>0.697457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.699632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.699157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.697175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.695483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.694343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.695115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.695419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.632600</td>\n",
       "      <td>0.693594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.692319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.631700</td>\n",
       "      <td>0.693440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.637200</td>\n",
       "      <td>0.694457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.644100</td>\n",
       "      <td>0.695495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>0.697030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.695969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.695796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.883500</td>\n",
       "      <td>0.695846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.621000</td>\n",
       "      <td>0.696998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>0.700168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>0.702794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>0.705509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.633100</td>\n",
       "      <td>0.710531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.713530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.713086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.710419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.708486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.708942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.639100</td>\n",
       "      <td>0.708644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.709748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.622300</td>\n",
       "      <td>0.707587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>0.704986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.702926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.702564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.603300</td>\n",
       "      <td>0.703084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.704391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.705917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.707557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.709208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>0.710134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.621100</td>\n",
       "      <td>0.710645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.710925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>0.710860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.620100</td>\n",
       "      <td>0.711001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.600200</td>\n",
       "      <td>0.711142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/16 23:39:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run stylish-cod-396 at: https://5bb1-156-204-128-49.ngrok-free.app/#/experiments/1/runs/b735ae6d403e4f39bbbfc67860b77b58.\n",
      "2024/11/16 23:39:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://5bb1-156-204-128-49.ngrok-free.app/#/experiments/1.\n"
     ]
    }
   ],
   "source": [
    "# Configure and train the model with specified parameters\n",
    "model = configure_and_train_model(\n",
    "    r=128,                 # LoRA rank parameter\n",
    "    lora_alpha=16,        # Scaling factor for LoRA layers\n",
    "    lora_dropout=0.1,       # Set dropout to 0 (optimized)\n",
    "    use_gradient_checkpointing=\"none\",  # Memory optimization\n",
    "    random_state=3407,    # Seed for reproducibility\n",
    "    use_rslora=False,     # Disable rank-stabilized LoRA\n",
    "    loftq_config=None,    # Leave as None for default\n",
    "    train_dataset=train_data,\n",
    "    valid_dataset=valid_data,\n",
    "    batch_size=2,         # Set batch size for training\n",
    "    grad_accum_steps=16,   # Gradient accumulation steps\n",
    "    warmup_steps=10,       # Warmup steps for learning rate scheduler\n",
    "    max_steps=150,         # Total steps for quick testing\n",
    "    learning_rate=2e-3,   # Learning rate\n",
    "    weight_decay=0.01,    # Weight decay for optimizer\n",
    "    lr_scheduler_type=\"linear\",  # Learning rate scheduler type\n",
    "    output_dir=\"outputs\"  # Directory for saving results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:39:42.223607Z",
     "iopub.status.busy": "2024-11-16T23:39:42.223137Z",
     "iopub.status.idle": "2024-11-16T23:39:42.235456Z",
     "shell.execute_reply": "2024-11-16T23:39:42.234481Z",
     "shell.execute_reply.started": "2024-11-16T23:39:42.223558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak reserved memory = 11.752 GB.\n",
      "Peak reserved memory for training = 9.447 GB.\n",
      "Peak reserved memory % of max memory = 79.723 %.\n",
      "Peak reserved memory for training % of max memory = 64.087 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:39:44.091497Z",
     "iopub.status.busy": "2024-11-16T23:39:44.090488Z",
     "iopub.status.idle": "2024-11-16T23:39:44.099152Z",
     "shell.execute_reply": "2024-11-16T23:39:44.098155Z",
     "shell.execute_reply.started": "2024-11-16T23:39:44.091450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\n",
    "The best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\n",
    "\n",
    "### DESCRIPTION:\n",
    "{}\n",
    "\n",
    "### RESPONSE:\"\"\"\n",
    "\n",
    "\n",
    "def formatting_test_prompts_func(examples):\n",
    "    global tokenizer\n",
    "\n",
    "    inputs = examples[\"series_description\"]\n",
    "    texts = []\n",
    "    for input in  inputs:\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = test_prompt.format( input)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:39:45.915866Z",
     "iopub.status.busy": "2024-11-16T23:39:45.914951Z",
     "iopub.status.idle": "2024-11-16T23:39:45.953419Z",
     "shell.execute_reply": "2024-11-16T23:39:45.952567Z",
     "shell.execute_reply.started": "2024-11-16T23:39:45.915821Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30e7ea569dc4843966cfa8d5ebcb3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n",
       "    num_rows: 83\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test_data.map(formatting_test_prompts_func, batched = True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:39:47.782955Z",
     "iopub.status.busy": "2024-11-16T23:39:47.782155Z",
     "iopub.status.idle": "2024-11-16T23:39:50.870757Z",
     "shell.execute_reply": "2024-11-16T23:39:50.869464Z",
     "shell.execute_reply.started": "2024-11-16T23:39:47.782912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\nThe best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\\n\\n### DESCRIPTION:\\nA univariate time-series dataset  consists of 48 samples with a missing values percentage of 0.0% imputed using FBProphet model and 0.0% detected outliers. The target series has a sampling rate of 1440 minutes, minimum value of -1.0577518085465023, maximum value of 0.5039540640878073, median value of -0.5925601507310199, mean value of -0.49076897775897593, and average standard deviation of 0.21373626312059368 for the 10 percentiles. The series is detected as non-stationary using dickey fuller testand it turns into a stationary series using first order differencing. The series has 6 significant lags observed using the partial autocorrelation function (pACF), the pACF values for these 6 lags ranges from 0.6091821079108509 for the lag number 1 to 0.3608509812237311 for the lag number 7. There exist 1 insignificant lags at these indices 4 between the first and the last significant ones. The series looks to be a multiplicative time-series with a logistic trend. There are no seasonality components detected in the seriesThe series exhibits a skewness value of 0.6670990699653278 and a kurtosis value of 0.47278337091329936. The Fractal dimension analysis yields a value of -0.4583557992511385, indicating a Complex and irregular time-series structure. The dataset is converted into a simple regression task by extracting the previously described features.\\n\\n### RESPONSE:<eos>XGBoostRegressor\\n\\n### DESCRIPTION:\\nA univariate time-series dataset  consists of 48 samples with a missing values percentage of 0.0% imputed using FBProphet model and 0.0% detected outliers. The target series has a sampling rate of 1440 minutes, minimum']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[test_dataset['text'][0]], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:39:55.973370Z",
     "iopub.status.busy": "2024-11-16T23:39:55.972500Z",
     "iopub.status.idle": "2024-11-16T23:39:56.392204Z",
     "shell.execute_reply": "2024-11-16T23:39:56.391376Z",
     "shell.execute_reply.started": "2024-11-16T23:39:55.973329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\nThe best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\\n\\n### DESCRIPTION:\\nA univariate time-series dataset  consists of 81 samples with a missing values percentage of 0.0% imputed using FBProphet model and 0.0% detected outliers. The target series has a sampling rate of 44640 minutes, minimum value of 3523.548387096774, maximum value of 6434.0, median value of 4978.387096774193, mean value of 4995.464271746457, and average standard deviation of 0.06947531163920204 for the 10 percentiles. The series is detected as stationary using dickey fuller test.The series has 7 significant lags observed using the partial autocorrelation function (pACF), the pACF values for these 7 lags ranges from 0.9375534431746181 for the lag number 1 to 0.2340147204900702 for the lag number 7.There are no insignificant lags in between the first and the last significant ones.The series looks to be a multiplicative time-series with a linear trend. There are 1 seasonality components detected in this series represented as sinusoidal waves with periods 27.The series exhibits a skewness value of 0.018459508790673103 and a kurtosis value of 1.0220489598846814. The Fractal dimension analysis yields a value of -0.07028093361365781, indicating a Complex and irregular time-series structure. The dataset is converted into a simple regression task by extracting the previously described features.\\n\\n### RESPONSE:<eos>ADABoostRegressor\\n\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[test_dataset['text'][1]], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 5, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:40:05.642647Z",
     "iopub.status.busy": "2024-11-16T23:40:05.642253Z",
     "iopub.status.idle": "2024-11-16T23:40:37.463840Z",
     "shell.execute_reply": "2024-11-16T23:40:37.463000Z",
     "shell.execute_reply.started": "2024-11-16T23:40:05.642611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_responses=[]\n",
    "# get all test data inference result\n",
    "for test_prompt in test_dataset['text']:\n",
    "  inputs= tokenizer(\n",
    "  [test_prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "  outputs = model.generate(**inputs, max_new_tokens = 5, use_cache = True)\n",
    "  test_responses.append(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:40:40.890317Z",
     "iopub.status.busy": "2024-11-16T23:40:40.889388Z",
     "iopub.status.idle": "2024-11-16T23:40:40.928344Z",
     "shell.execute_reply": "2024-11-16T23:40:40.927489Z",
     "shell.execute_reply.started": "2024-11-16T23:40:40.890274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to a Pandas DataFrame\n",
    "df = test_dataset.to_pandas()\n",
    "df['model_responses']= test_responses\n",
    "df.to_csv('test_model_result_unsloth.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:04.602329Z",
     "iopub.status.busy": "2024-11-16T23:51:04.601388Z",
     "iopub.status.idle": "2024-11-16T23:51:04.614642Z",
     "shell.execute_reply": "2024-11-16T23:51:04.613591Z",
     "shell.execute_reply.started": "2024-11-16T23:51:04.602283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>ElasticNetRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>LassoRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>LightgbmRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>AdaboostRegressor',\n",
       " '<eos>XGBoostRegressor',\n",
       " '<eos>LightgbmRegressor']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for response in test_responses:\n",
    "    result = response[0].split('\\n\\n### RESPONSE:')[1].split('</s>')[0].strip()\n",
    "    result = result.replace('\\n', '').replace('.', '')  # Remove \\n and .\n",
    "    if result == '<eos>LIGHTgbmRegressor':\n",
    "        result = '<eos>LightgbmRegressor'\n",
    "    elif result == '<eos>ADABoostRegressor':\n",
    "        result = '<eos>AdaboostRegressor'\n",
    "        \n",
    "    predictions.append(result)\n",
    "\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:11.159538Z",
     "iopub.status.busy": "2024-11-16T23:51:11.158837Z",
     "iopub.status.idle": "2024-11-16T23:51:11.168499Z",
     "shell.execute_reply": "2024-11-16T23:51:11.167527Z",
     "shell.execute_reply.started": "2024-11-16T23:51:11.159480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'ElasticNetRegressor', 'LightgbmRegressor', 'LightgbmRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'LassoRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'LightgbmRegressor']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_model_name(prediction):\n",
    "    \"\"\"\n",
    "    Extract the model name from a prediction string using regex.\n",
    "    Corrects 'ADABoostRegressor' to 'AdaBoostRegressor'.\n",
    "    \"\"\"\n",
    "    # Adjust regex to match model names more precisely\n",
    "    match = re.search(r\"\\b[A-Za-z]+(?:[A-Za-z0-9]*)\\b\", prediction)\n",
    "    if match:\n",
    "        model_name = match.group()\n",
    "        return model_name\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Apply the extraction to all predictions\n",
    "extracted_predictions = [extract_model_name(pred.split('<eos>')[-1].strip()) for pred in predictions]\n",
    "\n",
    "# Print the extracted model names\n",
    "print(extracted_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:16.485670Z",
     "iopub.status.busy": "2024-11-16T23:51:16.484705Z",
     "iopub.status.idle": "2024-11-16T23:51:16.493170Z",
     "shell.execute_reply": "2024-11-16T23:51:16.492273Z",
     "shell.execute_reply.started": "2024-11-16T23:51:16.485627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:17.197507Z",
     "iopub.status.busy": "2024-11-16T23:51:17.196543Z",
     "iopub.status.idle": "2024-11-16T23:51:17.205302Z",
     "shell.execute_reply": "2024-11-16T23:51:17.204366Z",
     "shell.execute_reply.started": "2024-11-16T23:51:17.197467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data= df['algorithm']\n",
    "len(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:18.122118Z",
     "iopub.status.busy": "2024-11-16T23:51:18.120985Z",
     "iopub.status.idle": "2024-11-16T23:51:18.132077Z",
     "shell.execute_reply": "2024-11-16T23:51:18.130978Z",
     "shell.execute_reply.started": "2024-11-16T23:51:18.122064Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     GaussianProcessRegressor\n",
       "1     GaussianProcessRegressor\n",
       "2     GaussianProcessRegressor\n",
       "3          ElasticNetRegressor\n",
       "4             XGBoostRegressor\n",
       "                ...           \n",
       "78         ElasticNetRegressor\n",
       "79            XGBoostRegressor\n",
       "80           AdaboostRegressor\n",
       "81           AdaboostRegressor\n",
       "82              LassoRegressor\n",
       "Name: algorithm, Length: 83, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:18.907934Z",
     "iopub.status.busy": "2024-11-16T23:51:18.907477Z",
     "iopub.status.idle": "2024-11-16T23:51:18.921618Z",
     "shell.execute_reply": "2024-11-16T23:51:18.920454Z",
     "shell.execute_reply.started": "2024-11-16T23:51:18.907893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.1566265060240964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming `predictions` and `actual_data` are lists or arrays of labels\n",
    "f1 = f1_score(actual_data, extracted_predictions, average='micro')  # Use 'macro' or 'micro' as needed\n",
    "\n",
    "\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:20.687032Z",
     "iopub.status.busy": "2024-11-16T23:51:20.686316Z",
     "iopub.status.idle": "2024-11-16T23:51:20.696764Z",
     "shell.execute_reply": "2024-11-16T23:51:20.695589Z",
     "shell.execute_reply.started": "2024-11-16T23:51:20.686989Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1566265060240964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(actual_data, extracted_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save tuned model\n",
    "\n",
    "To save the final model as LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:26.901075Z",
     "iopub.status.busy": "2024-11-16T23:51:26.900203Z",
     "iopub.status.idle": "2024-11-16T23:51:29.053300Z",
     "shell.execute_reply": "2024-11-16T23:51:29.052269Z",
     "shell.execute_reply.started": "2024-11-16T23:51:26.901033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local saving\n",
    "\n",
    "model.save_pretrained(\"lora_model\")\n",
    "\n",
    "tokenizer.save_pretrained(\"lora_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:29.056114Z",
     "iopub.status.busy": "2024-11-16T23:51:29.055362Z",
     "iopub.status.idle": "2024-11-16T23:51:36.650222Z",
     "shell.execute_reply": "2024-11-16T23:51:36.649447Z",
     "shell.execute_reply.started": "2024-11-16T23:51:29.056066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319b736658444b74ab7f75433b7e61f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b03f1fa0b94236b0c2eb992f8cf792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4109eae41c46464190fb3a259268f81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/628M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/RanaHossny213/gamma_tuned-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "# Online saving on HF\n",
    "from huggingface_hub import login\n",
    "\n",
    "new_model_adabtor= \"RanaHossny213/gamma_tuned-2b\"\n",
    "login(token=\"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\")  # Use your Hugging Face token\n",
    "\n",
    "# Push the model and tokenizer to the Hugging Face hub\n",
    "model.push_to_hub(new_model_adabtor)\n",
    "tokenizer.push_to_hub(new_model_adabtor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:51:36.652288Z",
     "iopub.status.busy": "2024-11-16T23:51:36.651961Z",
     "iopub.status.idle": "2024-11-16T23:52:19.097932Z",
     "shell.execute_reply": "2024-11-16T23:52:19.097006Z",
     "shell.execute_reply.started": "2024-11-16T23:51:36.652252Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
      "This might take 5 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 10 minutes for Llama-7b... Done.\n",
      "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
      "This might take 5 minutes...\n",
      "Done.\n",
      "Unsloth: Saving 4bit Bitsandbytes model. Please wait...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafc8b0e72ea4a4fb8ec15caffa26f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f043a6890844cb8bc262ed20fa18a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9165d6e5f6d4f6b8bf1bf996d84e048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged_4bit model to https://huggingface.co/model\n"
     ]
    }
   ],
   "source": [
    "# Save and Merge to 4bit\n",
    "\n",
    "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\",token = \"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\")\n",
    "\n",
    "model.push_to_hub_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\", token = \"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T23:52:19.129480Z",
     "iopub.status.busy": "2024-11-16T23:52:19.129061Z",
     "iopub.status.idle": "2024-11-16T23:52:27.053173Z",
     "shell.execute_reply": "2024-11-16T23:52:27.052118Z",
     "shell.execute_reply.started": "2024-11-16T23:52:19.129434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... Done.\n",
      "Unsloth: Saving LoRA adapters. Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved lora model to https://huggingface.co/model\n"
     ]
    }
   ],
   "source": [
    "# Save just LoRA adapters\n",
    "\n",
    "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",token = \"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\")\n",
    "\n",
    "model.push_to_hub_merged(\"model\", tokenizer, save_method = \"lora\", token = \"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i couldnot store gguf becouse the storage of kaggle but i was stored old version of the model(not the best):\n",
    "https://huggingface.co/RanaHossny213/gamma-ft-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6060942,
     "sourceId": 9872956,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6087213,
     "sourceId": 9907752,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
