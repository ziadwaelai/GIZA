{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O8taXvMbLte"
      },
      "source": [
        "# install the needed libaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone --recursive https://github.com/ggerganov/llama.cpp\n",
        "# !cd llama.cpp && git submodule update --init --recursive\n",
        "# !pip install -r llama.cpp/requirements.txt\n",
        "# !pip install gradio \"huggingface-hub\" \"gradio_huggingfacehub_search==0.0.7\" \"APScheduler\"\n",
        "!cd llama.cpp && make clean && make -j all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPsL6kOFbTWU",
        "outputId": "a3be08cf-274f-41b5-e734-336f66c0ecfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX \n",
            "I NVCCFLAGS: -std=c++11 -O3 -g \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf libllava.a llama-batched llama-batched-bench llama-bench llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-minicpmv-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-simple-chat llama-speculative llama-tokenize llama-vdot llama-cvector-generator llama-gen-docs tests/test-c.o libggml.so libggml.a libllama.so libllama.a libcommon.so libcommon.a tests/test-arg-parser tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-log tests/test-model-load-cancel tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
            "rm -rvf *.a *.dll *.so *.dot\n",
            "find ggml src common tests examples pocs -type f -name \"*.o\" -delete\n",
            "find ggml src common tests examples pocs -type f -name \"*.d\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX \n",
            "I NVCCFLAGS: -std=c++11 -O3 -g \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml.c -o ggml/src/ggml.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml-aarch64.c -o ggml/src/ggml-aarch64.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml-alloc.c -o ggml/src/ggml-alloc.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-backend.cpp -o ggml/src/ggml-backend.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-backend-reg.cpp -o ggml/src/ggml-backend-reg.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-opt.cpp -o ggml/src/ggml-opt.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml-quants.c -o ggml/src/ggml-quants.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-threading.cpp -o ggml/src/ggml-threading.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml-cpu/ggml-cpu.c -o ggml/src/ggml-cpu/ggml-cpu.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX    -c ggml/src/ggml-cpu/ggml-cpu.cpp -o ggml/src/ggml-cpu/ggml-cpu-cpp.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml-cpu/ggml-cpu-aarch64.c -o ggml/src/ggml-cpu/ggml-cpu-aarch64.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -MMD -c ggml/src/ggml-cpu/ggml-cpu-quants.c -o ggml/src/ggml-cpu/ggml-cpu-quants.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-cpu/llamafile/sgemm.cpp -o ggml/src/ggml-cpu/llamafile/sgemm.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-amx/ggml-amx.cpp -o ggml/src/ggml-amx/ggml-amx.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c ggml/src/ggml-amx/mmq.cpp -o ggml/src/ggml-amx/mmq.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c src/llama.cpp -o src/llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c src/llama-vocab.cpp -o src/llama-vocab.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c src/llama-grammar.cpp -o src/llama-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c src/llama-sampling.cpp -o src/llama-sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c src/unicode.cpp -o src/unicode.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c src/unicode-data.cpp -o src/unicode-data.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/common.cpp -o common/common.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/arg.cpp -o common/arg.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/log.cpp -o common/log.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/console.cpp -o common/console.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/ngram-cache.cpp -o common/ngram-cache.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/sampling.cpp -o common/sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -MMD -c common/json-schema-to-grammar.cpp -o common/json-schema-to-grammar.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/sha1/sha1.c -o examples/gguf-hash/deps/sha1/sha1.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/xxhash/xxhash.c -o examples/gguf-hash/deps/xxhash/xxhash.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/sha256/sha256.c -o examples/gguf-hash/deps/sha256/sha256.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-double-float.cpp -o tests/test-double-float.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c common/build-info.cpp -o common/build-info.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  examples/deprecation-warning/deprecation-warning.o -o main  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  examples/deprecation-warning/deprecation-warning.o -o server  \n",
            "NOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\n",
            "NOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX   tests/test-double-float.o -o tests/test-double-float  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-backend-ops.cpp -o tests/test-backend-ops.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-quantize-fns.cpp -o tests/test-quantize-fns.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-quantize-perf.cpp -o tests/test-quantize-perf.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-rope.cpp -o tests/test-rope.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -shared -fPIC -o libggml.so ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o  \n",
            "ar rcs libggml.a ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o\n",
            "\u001b[01m\u001b[Ktests/test-quantize-fns.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kfloat dot_product_error(const ggml_type_traits*, const ggml_type_traits_cpu*, size_t, const float*, const float*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Ktests/test-quantize-fns.cpp:83:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kqfns\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   83 |     \u001b[01;35m\u001b[Kconst ggml_type_traits * qfns\u001b[m\u001b[K, const ggml_type_traits_cpu * qfns_cpu, size_t test_size, const float * test_data1, const float *test_data2\n",
            "      |     \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~^~~~\u001b[m\u001b[K\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o tests/test-rope.o -o tests/test-rope  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o tests/test-quantize-fns.o -o tests/test-quantize-fns  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o examples/gguf/gguf.o -o llama-gguf  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o pocs/vdot/q8dot.o -o llama-q8dot  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o pocs/vdot/vdot.o -o llama-vdot  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o tests/test-quantize-perf.o -o tests/test-quantize-perf  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o tests/test-backend-ops.o -o tests/test-backend-ops  \n",
            "ar rcs libcommon.a common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/main/main.cpp -o examples/main/main.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/gbnf-validator/gbnf-validator.cpp -o examples/gbnf-validator/gbnf-validator.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -Iexamples/gguf-hash/deps -c examples/gguf-hash/gguf-hash.cpp -o examples/gguf-hash/gguf-hash.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  examples/llava/llava-cli.cpp examples/llava/llava.cpp examples/llava/clip.cpp ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o -o llama-llava-cli   -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  examples/llava/minicpmv-cli.cpp examples/llava/llava.cpp examples/llava/clip.cpp ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o -o llama-minicpmv-cli   -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/server/server.cpp -o examples/server/server.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/simple-chat/simple-chat.cpp -o examples/simple-chat/simple-chat.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c examples/gen-docs/gen-docs.cpp -o examples/gen-docs/gen-docs.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-arg-parser.cpp -o tests/test-arg-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-autorelease.cpp -o tests/test-autorelease.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-chat-template.cpp -o tests/test-chat-template.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-grammar-integration.cpp -o tests/test-grammar-integration.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-grammar-parser.cpp -o tests/test-grammar-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -Iexamples/server -c tests/test-json-schema-to-grammar.cpp -o tests/test-json-schema-to-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-llama-grammar.cpp -o tests/test-llama-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-log.cpp -o tests/test-log.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-model-load-cancel.cpp -o tests/test-model-load-cancel.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-sampling.cpp -o tests/test-sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-tokenizer-0.cpp -o tests/test-tokenizer-0.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-tokenizer-1-bpe.cpp -o tests/test-tokenizer-1-bpe.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -c tests/test-tokenizer-1-spm.cpp -o tests/test-tokenizer-1-spm.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -shared -fPIC -o libllama.so src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o libggml.so  \n",
            "ar rcs libllama.a src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  tests/get-model.cpp ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-model-load-cancel.o -o tests/test-model-load-cancel  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-log.o -o tests/test-log  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  tests/get-model.cpp ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-autorelease.o -o tests/test-autorelease  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -shared -fPIC -o libcommon.so common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o libllama.so libggml.so  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/simple/simple.o -o llama-simple  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-merge.o -o llama-lookup-merge  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-tokenizer-1-spm.o -o tests/test-tokenizer-1-spm  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/tokenize/tokenize.o -o llama-tokenize  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/simple-chat/simple-chat.o -o llama-simple-chat  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-tokenizer-1-bpe.o -o tests/test-tokenizer-1-bpe  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/gbnf-validator/gbnf-validator.o -o llama-gbnf-validator  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-chat-template.o -o tests/test-chat-template  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  examples/gguf-hash/deps/sha1/sha1.o examples/gguf-hash/deps/xxhash/xxhash.o examples/gguf-hash/deps/sha256/sha256.o ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/gguf-hash/gguf-hash.o -o llama-gguf-hash  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/gguf-split/gguf-split.o -o llama-gguf-split  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/eval-callback/eval-callback.o -o llama-eval-callback  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-create.o -o llama-lookup-create  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/batched/batched.o -o llama-batched  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/batched-bench/batched-bench.o -o llama-batched-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-grammar-parser.o -o tests/test-grammar-parser  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/gen-docs/gen-docs.o -o llama-gen-docs  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/save-load-state/save-load-state.o -o llama-save-load-state  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/passkey/passkey.o -o llama-passkey  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/gritlm/gritlm.o -o llama-gritlm  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-stats.o -o llama-lookup-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup.o -o llama-lookup  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-llama-grammar.o -o tests/test-llama-grammar  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-sampling.o -o tests/test-sampling  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/embedding/embedding.o -o llama-embedding  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-tokenizer-0.o -o tests/test-tokenizer-0  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/lookahead/lookahead.o -o llama-lookahead  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/parallel/parallel.o -o llama-parallel  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/quantize/quantize.o -o llama-quantize  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-arg-parser.o -o tests/test-arg-parser  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/infill/infill.o -o llama-infill  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o llama-convert-llama2c-to-ggml  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/retrieval/retrieval.o -o llama-retrieval  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/export-lora/export-lora.o -o llama-export-lora  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/main/main.o -o llama-cli  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/speculative/speculative.o -o llama-speculative  \n",
            "\n",
            "====  Run ./llama-cli -h for help.  ====\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/imatrix/imatrix.o -o llama-imatrix  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/perplexity/perplexity.o -o llama-perplexity  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/quantize-stats/quantize-stats.o -o llama-quantize-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-grammar-integration.o -o tests/test-grammar-integration  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o tests/test-json-schema-to-grammar.o -o tests/test-json-schema-to-grammar  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CPU_AARCH64 -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  ggml/src/ggml.o ggml/src/ggml-aarch64.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-backend-reg.o ggml/src/ggml-opt.o ggml/src/ggml-quants.o ggml/src/ggml-threading.o ggml/src/ggml-cpu/ggml-cpu.o ggml/src/ggml-cpu/ggml-cpu-cpp.o ggml/src/ggml-cpu/ggml-cpu-aarch64.o ggml/src/ggml-cpu/ggml-cpu-quants.o ggml/src/ggml-cpu/llamafile/sgemm.o ggml/src/ggml-amx/ggml-amx.o ggml/src/ggml-amx/mmq.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llama-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o common/log.o common/console.o common/ngram-cache.o common/sampling.o common/build-info.o common/json-schema-to-grammar.o -Iexamples/server examples/server/server.o -o llama-server   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:13:21.073855Z",
          "iopub.status.busy": "2024-11-16T22:13:21.073001Z",
          "iopub.status.idle": "2024-11-16T22:17:02.146366Z",
          "shell.execute_reply": "2024-11-16T22:17:02.145219Z",
          "shell.execute_reply.started": "2024-11-16T22:13:21.073802Z"
        },
        "trusted": true,
        "id": "ReJLypjLbLtf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:17:46.557681Z",
          "iopub.status.busy": "2024-11-16T22:17:46.557374Z",
          "iopub.status.idle": "2024-11-16T22:18:00.155823Z",
          "shell.execute_reply": "2024-11-16T22:18:00.154318Z",
          "shell.execute_reply.started": "2024-11-16T22:17:46.557647Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLkmTRNhbLtg",
        "outputId": "371204b8-e2df-4b24-8787-c22a9a947fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.4/571.4 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mlflow --quiet\n",
        "!pip install pyngrok --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUcx8obhbLtg"
      },
      "source": [
        "# import the needed libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.157810Z",
          "iopub.status.busy": "2024-11-16T22:18:00.157447Z",
          "iopub.status.idle": "2024-11-16T22:18:00.163123Z",
          "shell.execute_reply": "2024-11-16T22:18:00.161600Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.157772Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrcC2-qwbLtg",
        "outputId": "051c0174-f40c-4226-9a90-62b3028bb983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.167927Z",
          "iopub.status.busy": "2024-11-16T22:18:00.167504Z",
          "iopub.status.idle": "2024-11-16T22:18:00.177794Z",
          "shell.execute_reply": "2024-11-16T22:18:00.176851Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.167881Z"
        },
        "trusted": true,
        "id": "2M4xri72bLth"
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bfloat16_supported\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.179768Z",
          "iopub.status.busy": "2024-11-16T22:18:00.179220Z",
          "iopub.status.idle": "2024-11-16T22:18:00.187611Z",
          "shell.execute_reply": "2024-11-16T22:18:00.186511Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.179725Z"
        },
        "trusted": true,
        "id": "Ts8O239ObLth"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import TrainerCallback\n",
        "import os\n",
        "from accelerate import Accelerator\n",
        "import re\n",
        "from pyngrok import ngrok\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "import time\n",
        "\n",
        "from trl import  DataCollatorForCompletionOnlyLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NzaP8DGbLth"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.189316Z",
          "iopub.status.busy": "2024-11-16T22:18:00.188972Z",
          "iopub.status.idle": "2024-11-16T22:18:00.197647Z",
          "shell.execute_reply": "2024-11-16T22:18:00.196459Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.189275Z"
        },
        "trusted": true,
        "id": "o1QdEtWpbLth"
      },
      "outputs": [],
      "source": [
        "train_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\n",
        "The best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\n",
        "\n",
        "### DESCRIPTION:\n",
        "{}\n",
        "\n",
        "### RESPONSE:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples,EOS_TOKEN):\n",
        "\n",
        "    inputs       = examples[\"series_description\"]\n",
        "\n",
        "    outputs      = examples[\"algorithm\"]\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for input, output in zip( inputs, outputs):\n",
        "\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "\n",
        "        text = train_prompt.format( input, output) + EOS_TOKEN\n",
        "\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.199896Z",
          "iopub.status.busy": "2024-11-16T22:18:00.199125Z",
          "iopub.status.idle": "2024-11-16T22:18:00.526559Z",
          "shell.execute_reply": "2024-11-16T22:18:00.525580Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.199854Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "55c2de2d3b4545bcb52d0ec34c955894"
          ]
        },
        "id": "-Y0qEsIybLth",
        "outputId": "68040b90-cad5-488c-d494-cfa5f5e1fc89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55c2de2d3b4545bcb52d0ec34c955894",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters'],\n",
              "        num_rows: 828\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset('csv', data_files=\"/kaggle/input/regression-univariate-train/Regression_Univariate_train.csv\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.528355Z",
          "iopub.status.busy": "2024-11-16T22:18:00.527928Z",
          "iopub.status.idle": "2024-11-16T22:18:00.555380Z",
          "shell.execute_reply": "2024-11-16T22:18:00.554409Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.528308Z"
        },
        "trusted": true,
        "id": "2mFPMosibLti"
      },
      "outputs": [],
      "source": [
        "train_data = dataset['train']\n",
        "\n",
        "# Step 1: Split into 80% train and 20% remaining (val + test)\n",
        "train_valid_test_split = train_data.train_test_split(test_size=0.2, seed=42)\n",
        "train_data = train_valid_test_split['train']\n",
        "remaining_data = train_valid_test_split['test']\n",
        "\n",
        "# Step 2: Split the remaining data into 50% validation and 50% test\n",
        "valid_test_split = remaining_data.train_test_split(test_size=0.5, seed=42)\n",
        "valid_data = valid_test_split['train']\n",
        "test_data = valid_test_split['test']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0omK3p-sbLti"
      },
      "source": [
        "# define and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:00.557501Z",
          "iopub.status.busy": "2024-11-16T22:18:00.556821Z",
          "iopub.status.idle": "2024-11-16T22:18:29.453894Z",
          "shell.execute_reply": "2024-11-16T22:18:29.453021Z",
          "shell.execute_reply.started": "2024-11-16T22:18:00.557453Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "2c21111e82084163a06bf0d969566804",
            "931727f47e1341de97aa9958bf68948b",
            "e8a4b7e8fe0e43aaad2b84b64906d72f",
            "783d4bb3e9ab4c0db113bd0fc0e16390",
            "81f80dc190a0466dba098a53042c5bc5",
            "151438edee1740d8b5afc0d57224b492"
          ]
        },
        "id": "CP0XgsJsbLti",
        "outputId": "6fc4c713-14df-44f4-f22f-0aa4720578a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2024.11.7: Fast Gemma patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c21111e82084163a06bf0d969566804",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.01G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "931727f47e1341de97aa9958bf68948b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8a4b7e8fe0e43aaad2b84b64906d72f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "783d4bb3e9ab4c0db113bd0fc0e16390",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81f80dc190a0466dba098a53042c5bc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "151438edee1740d8b5afc0d57224b492",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "max_seq_length = 2048# Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2b\",\n",
        "    max_seq_length # Add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "= max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:29.456105Z",
          "iopub.status.busy": "2024-11-16T22:18:29.455394Z",
          "iopub.status.idle": "2024-11-16T22:18:29.462012Z",
          "shell.execute_reply": "2024-11-16T22:18:29.461028Z",
          "shell.execute_reply.started": "2024-11-16T22:18:29.456057Z"
        },
        "trusted": true,
        "id": "ufC8OUyHbLti"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.add_eos_token = True\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer_config = {\n",
        "            \"Tokenizer\": tokenizer.__class__.__name__,\n",
        "            \"padding_side\": tokenizer.padding_side,\n",
        "            \"add_eos_token\": tokenizer.add_eos_token,\n",
        "            \"pad_token\": tokenizer.pad_token\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:29.463682Z",
          "iopub.status.busy": "2024-11-16T22:18:29.463321Z",
          "iopub.status.idle": "2024-11-16T22:18:33.903984Z",
          "shell.execute_reply": "2024-11-16T22:18:33.902960Z",
          "shell.execute_reply.started": "2024-11-16T22:18:29.463647Z"
        },
        "trusted": true,
        "id": "tcwkSX9wbLtj"
      },
      "outputs": [],
      "source": [
        "instruction_template=\"DESCRIPTION:\"\n",
        "\n",
        "# Set MLflow's tracking URI and experiment\n",
        "mlflow.set_tracking_uri(\"https://5bb1-156-204-128-49.ngrok-free.app\")\n",
        "mlflow.set_experiment(\"best model gamma-2b\")\n",
        "mlflow.pytorch.autolog(disable=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:33.905591Z",
          "iopub.status.busy": "2024-11-16T22:18:33.905239Z",
          "iopub.status.idle": "2024-11-16T22:18:33.914154Z",
          "shell.execute_reply": "2024-11-16T22:18:33.913186Z",
          "shell.execute_reply.started": "2024-11-16T22:18:33.905553Z"
        },
        "trusted": true,
        "id": "6vb9xsjSbLtj"
      },
      "outputs": [],
      "source": [
        "class MLFlowLoggingCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.training_loss = []\n",
        "        self.eval_loss = []\n",
        "\n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        # Initialize logging at the start of training\n",
        "        print(\"Training started.\")\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Log training and evaluation losses\n",
        "        if 'loss' in logs:\n",
        "            self.training_loss.append(logs['loss'])\n",
        "            mlflow.log_metric(\"training_loss\", logs['loss'], step=state.global_step)\n",
        "\n",
        "        if 'eval_loss' in logs:\n",
        "            self.eval_loss.append(logs['eval_loss'])\n",
        "            mlflow.log_metric(\"validation_loss\", logs['eval_loss'], step=state.global_step)\n",
        "\n",
        "        if 'eval_f1' in logs:\n",
        "            mlflow.log_metric(\"validation_f1\", logs['eval_f1'], step=state.global_step)\n",
        "\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        # Log final results at the end of training\n",
        "        print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:33.916007Z",
          "iopub.status.busy": "2024-11-16T22:18:33.915641Z",
          "iopub.status.idle": "2024-11-16T22:18:33.931007Z",
          "shell.execute_reply": "2024-11-16T22:18:33.930179Z",
          "shell.execute_reply.started": "2024-11-16T22:18:33.915963Z"
        },
        "trusted": true,
        "id": "At9mbzGDbLtj"
      },
      "outputs": [],
      "source": [
        "def configure_and_train_model(\n",
        "    r=128,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    use_gradient_checkpointing=\"none\",\n",
        "    random_state=2048,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        "    train_dataset=None,\n",
        "    valid_dataset=None,\n",
        "    batch_size=2,\n",
        "    grad_accum_steps=16,\n",
        "    warmup_steps=5,\n",
        "    max_steps=-1,\n",
        "    learning_rate=2e-3,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    output_dir=\"outputs\"\n",
        "):\n",
        "    global model\n",
        "    # Load model and tokenizer with specified settings\n",
        "    lora_config = {\n",
        "        \"r\": r,\n",
        "        \"lora_alpha\": lora_alpha,\n",
        "        \"lora_dropout\": lora_dropout,\n",
        "        \"use_gradient_checkpointing\": use_gradient_checkpointing,\n",
        "        \"random_state\": random_state,\n",
        "        \"use_rslora\": use_rslora,\n",
        "        \"loftq_config\": loftq_config,\n",
        "    }\n",
        "    with mlflow.start_run():\n",
        "      mlflow.set_tag(\"model_name\", \"gamma-2b\")\n",
        "      # Format datasets with EOS token\n",
        "      train_dataset = train_dataset.map(\n",
        "            lambda batch: formatting_prompts_func(batch, EOS_TOKEN=tokenizer.eos_token),\n",
        "            batched=True\n",
        "        )\n",
        "      valid_dataset = valid_dataset.map(\n",
        "            lambda batch: formatting_prompts_func(batch, EOS_TOKEN=tokenizer.eos_token),\n",
        "            batched=True\n",
        "        )\n",
        "\n",
        "      # Configure PEFT model\n",
        "      model = FastLanguageModel.get_peft_model(\n",
        "          model,\n",
        "          r=r,\n",
        "          target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "          lora_alpha=lora_alpha,\n",
        "          lora_dropout=lora_dropout,\n",
        "          bias=\"none\",\n",
        "          use_gradient_checkpointing=use_gradient_checkpointing,\n",
        "          random_state=random_state,\n",
        "          use_rslora=use_rslora,\n",
        "          loftq_config=loftq_config\n",
        "      )\n",
        "\n",
        "\n",
        "      # Training arguments\n",
        "      training_args = TrainingArguments(\n",
        "          per_device_train_batch_size=batch_size,\n",
        "          gradient_accumulation_steps=grad_accum_steps,\n",
        "          warmup_steps=warmup_steps,\n",
        "          max_steps=max_steps,\n",
        "          learning_rate=learning_rate,\n",
        "          fp16=not is_bfloat16_supported(),\n",
        "          bf16=is_bfloat16_supported(),\n",
        "          logging_steps=1,\n",
        "          optim=\"adamw_8bit\",\n",
        "          weight_decay=weight_decay,\n",
        "          lr_scheduler_type=lr_scheduler_type,\n",
        "          seed=random_state,\n",
        "          output_dir=output_dir,\n",
        "          eval_strategy=\"steps\",\n",
        "          save_strategy=\"epoch\"\n",
        "      )\n",
        "      mlflow.log_params(vars(training_args))\n",
        "\n",
        "      mlflow.log_params(tokenizer_config)\n",
        "      mlflow.log_params(lora_config)\n",
        "      # Trainer setup\n",
        "      trainer = SFTTrainer(\n",
        "          model=model,\n",
        "          tokenizer=tokenizer,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=valid_dataset,\n",
        "          dataset_text_field=\"text\",\n",
        "          max_seq_length=2048,\n",
        "          dataset_num_proc=2,\n",
        "          packing=False,\n",
        "          args=training_args\n",
        "          )\n",
        "      # Initialize the callback\n",
        "      mlflow_callback = MLFlowLoggingCallback()\n",
        "\n",
        "      # Train the model with the callback\n",
        "      trainer.add_callback(mlflow_callback)\n",
        "      # Train the model\n",
        "      trainer.train()\n",
        "      return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:33.932372Z",
          "iopub.status.busy": "2024-11-16T22:18:33.932063Z",
          "iopub.status.idle": "2024-11-16T22:18:33.942446Z",
          "shell.execute_reply": "2024-11-16T22:18:33.941564Z",
          "shell.execute_reply.started": "2024-11-16T22:18:33.932340Z"
        },
        "trusted": true,
        "id": "yzfaS1FZbLtj",
        "outputId": "1fd3c3f6-8652-4bad-d98f-a0436cdc6fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "2.305 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Show current memory stats\n",
        "\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T22:18:33.943911Z",
          "iopub.status.busy": "2024-11-16T22:18:33.943645Z",
          "iopub.status.idle": "2024-11-16T23:39:34.021314Z",
          "shell.execute_reply": "2024-11-16T23:39:34.020279Z",
          "shell.execute_reply.started": "2024-11-16T22:18:33.943882Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "f4e340a89ba34c579883ec4d419e3e8b",
            "ba26f2a5892542c3ae21aa5bfa38d1aa",
            "9e419970e0e84e1c9d6b5694678d066c",
            "8c8be0888ac3434290829b226c6cc010",
            "c9c958b3799a478ca9588fb02edb804c"
          ]
        },
        "id": "HAAtoBUEbLtk",
        "outputId": "a98b2644-25ea-4059-91a1-a8598ae2ba5c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4e340a89ba34c579883ec4d419e3e8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/662 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba26f2a5892542c3ae21aa5bfa38d1aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2024.11.7 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e419970e0e84e1c9d6b5694678d066c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/662 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c8be0888ac3434290829b226c6cc010",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 662 | Num Epochs = 8\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 16\n",
            "\\        /    Total batch size = 32 | Total steps = 150\n",
            " \"-____-\"     Number of trainable parameters = 156,893,184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "2024/11/16 22:18:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id b735ae6d403e4f39bbbfc67860b77b58: Failed to log run data: Exception: INVALID_PARAMETER_VALUE: Changing param values is not allowed. Params were already logged='[{'key': 'logging_strategy', 'old_value': 'IntervalStrategy.STEPS', 'new_value': 'steps'}, {'key': 'save_strategy', 'old_value': 'IntervalStrategy.EPOCH', 'new_value': 'epoch'}, {'key': 'accelerator_config', 'old_value': 'AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False)', 'new_value': \"{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\"}, {'key': 'optim', 'old_value': 'OptimizerNames.ADAMW_8BIT', 'new_value': 'adamw_8bit'}, {'key': 'hub_strategy', 'old_value': 'HubStrategy.EVERY_SAVE', 'new_value': 'every_save'}, {'key': 'hub_token', 'old_value': 'None', 'new_value': '<HUB_TOKEN>'}, {'key': 'push_to_hub_token', 'old_value': 'None', 'new_value': '<PUSH_TO_HUB_TOKEN>'}]' for run ID='b735ae6d403e4f39bbbfc67860b77b58'.\n",
            "2024/11/16 22:18:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id b735ae6d403e4f39bbbfc67860b77b58: Failed to log run data: Exception: INVALID_PARAMETER_VALUE: Changing param values is not allowed. Params were already logged='[{'key': 'eval_strategy', 'old_value': 'IntervalStrategy.STEPS', 'new_value': 'steps'}, {'key': 'lr_scheduler_type', 'old_value': 'SchedulerType.LINEAR', 'new_value': 'linear'}]' for run ID='b735ae6d403e4f39bbbfc67860b77b58'.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9c958b3799a478ca9588fb02edb804c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113925900003273, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/kaggle/working/wandb/run-20241116_221906-1wtfk1bc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ranahossny52/huggingface/runs/1wtfk1bc' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/ranahossny52/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ranahossny52/huggingface' target=\"_blank\">https://wandb.ai/ranahossny52/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ranahossny52/huggingface/runs/1wtfk1bc' target=\"_blank\">https://wandb.ai/ranahossny52/huggingface/runs/1wtfk1bc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 1:20:04, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.472800</td>\n",
              "      <td>2.485648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.470500</td>\n",
              "      <td>2.388825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.382500</td>\n",
              "      <td>2.171220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.151600</td>\n",
              "      <td>1.922029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.912500</td>\n",
              "      <td>1.614556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.620700</td>\n",
              "      <td>1.273228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.283100</td>\n",
              "      <td>1.013885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.017400</td>\n",
              "      <td>0.914271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.923200</td>\n",
              "      <td>0.856998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.870500</td>\n",
              "      <td>0.815456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.822400</td>\n",
              "      <td>0.819695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.824300</td>\n",
              "      <td>0.768387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.761900</td>\n",
              "      <td>0.746063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.771700</td>\n",
              "      <td>0.732625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.759500</td>\n",
              "      <td>0.729681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>0.724641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.751400</td>\n",
              "      <td>0.719430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.726100</td>\n",
              "      <td>0.730582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.726700</td>\n",
              "      <td>0.719433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.710300</td>\n",
              "      <td>0.716428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.166100</td>\n",
              "      <td>0.716105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.727200</td>\n",
              "      <td>0.713933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.698100</td>\n",
              "      <td>0.709693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.728900</td>\n",
              "      <td>0.715757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.715100</td>\n",
              "      <td>0.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.710100</td>\n",
              "      <td>0.706621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.721100</td>\n",
              "      <td>0.707550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.712000</td>\n",
              "      <td>0.704894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.706000</td>\n",
              "      <td>0.703965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.724900</td>\n",
              "      <td>0.704222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.695700</td>\n",
              "      <td>0.702540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.740700</td>\n",
              "      <td>0.699151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.698700</td>\n",
              "      <td>0.697037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.711900</td>\n",
              "      <td>0.697341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.697901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.700600</td>\n",
              "      <td>0.696833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.714100</td>\n",
              "      <td>0.695272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.680800</td>\n",
              "      <td>0.693980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.721000</td>\n",
              "      <td>0.693614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.720500</td>\n",
              "      <td>0.693298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.123300</td>\n",
              "      <td>0.692140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>0.692832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.701200</td>\n",
              "      <td>0.691873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.709000</td>\n",
              "      <td>0.691275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.695200</td>\n",
              "      <td>0.690688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.697000</td>\n",
              "      <td>0.689440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.683500</td>\n",
              "      <td>0.689091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.689900</td>\n",
              "      <td>0.690408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.679700</td>\n",
              "      <td>0.689292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.691400</td>\n",
              "      <td>0.688006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.663900</td>\n",
              "      <td>0.688357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.670600</td>\n",
              "      <td>0.688918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.677700</td>\n",
              "      <td>0.688681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.699300</td>\n",
              "      <td>0.687339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.670500</td>\n",
              "      <td>0.687114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.697500</td>\n",
              "      <td>0.686865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.699700</td>\n",
              "      <td>0.685639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.672800</td>\n",
              "      <td>0.685172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.684100</td>\n",
              "      <td>0.684496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.685800</td>\n",
              "      <td>0.683813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.980700</td>\n",
              "      <td>0.682681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.838400</td>\n",
              "      <td>0.684156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.675900</td>\n",
              "      <td>0.687477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.669100</td>\n",
              "      <td>0.689633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.689570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.656000</td>\n",
              "      <td>0.686774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.652300</td>\n",
              "      <td>0.683844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.665500</td>\n",
              "      <td>0.682431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.663400</td>\n",
              "      <td>0.682402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.661200</td>\n",
              "      <td>0.684787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.678700</td>\n",
              "      <td>0.687397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.674100</td>\n",
              "      <td>0.686908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.688300</td>\n",
              "      <td>0.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.681716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.678200</td>\n",
              "      <td>0.681116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.666100</td>\n",
              "      <td>0.681434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.664500</td>\n",
              "      <td>0.681873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.669000</td>\n",
              "      <td>0.682291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.683100</td>\n",
              "      <td>0.682909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.668700</td>\n",
              "      <td>0.683238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>0.681638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.074200</td>\n",
              "      <td>0.679967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.692000</td>\n",
              "      <td>0.680728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.651500</td>\n",
              "      <td>0.682898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.656600</td>\n",
              "      <td>0.688031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.634000</td>\n",
              "      <td>0.692565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.671400</td>\n",
              "      <td>0.691892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.660900</td>\n",
              "      <td>0.687042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.684062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.657900</td>\n",
              "      <td>0.682888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.673600</td>\n",
              "      <td>0.684885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.623100</td>\n",
              "      <td>0.688826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.660300</td>\n",
              "      <td>0.690210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.653800</td>\n",
              "      <td>0.686716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.668600</td>\n",
              "      <td>0.684567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.656800</td>\n",
              "      <td>0.684574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.667200</td>\n",
              "      <td>0.683931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.643000</td>\n",
              "      <td>0.684451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.645300</td>\n",
              "      <td>0.684798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.668700</td>\n",
              "      <td>0.685114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.629200</td>\n",
              "      <td>0.685076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.061300</td>\n",
              "      <td>0.683922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.648900</td>\n",
              "      <td>0.686083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.623800</td>\n",
              "      <td>0.690461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.635600</td>\n",
              "      <td>0.692952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.636600</td>\n",
              "      <td>0.695786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.634800</td>\n",
              "      <td>0.697457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.657300</td>\n",
              "      <td>0.699632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.632700</td>\n",
              "      <td>0.699157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.643000</td>\n",
              "      <td>0.697175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.635600</td>\n",
              "      <td>0.695483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.641200</td>\n",
              "      <td>0.694343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.630100</td>\n",
              "      <td>0.695115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.642600</td>\n",
              "      <td>0.695419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.632600</td>\n",
              "      <td>0.693594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.652700</td>\n",
              "      <td>0.692319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.631700</td>\n",
              "      <td>0.693440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.637200</td>\n",
              "      <td>0.694457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.644100</td>\n",
              "      <td>0.695495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.649400</td>\n",
              "      <td>0.697030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.633400</td>\n",
              "      <td>0.695969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.786000</td>\n",
              "      <td>0.695796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.883500</td>\n",
              "      <td>0.695846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.621000</td>\n",
              "      <td>0.696998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.630900</td>\n",
              "      <td>0.700168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.616300</td>\n",
              "      <td>0.702794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.625400</td>\n",
              "      <td>0.705509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.633100</td>\n",
              "      <td>0.710531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.609600</td>\n",
              "      <td>0.713530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.611600</td>\n",
              "      <td>0.713086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.596800</td>\n",
              "      <td>0.710419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.614300</td>\n",
              "      <td>0.708486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.637500</td>\n",
              "      <td>0.708942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.639100</td>\n",
              "      <td>0.708644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.616500</td>\n",
              "      <td>0.709748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.622300</td>\n",
              "      <td>0.707587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.626300</td>\n",
              "      <td>0.704986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.617800</td>\n",
              "      <td>0.702926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.644200</td>\n",
              "      <td>0.702564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.603300</td>\n",
              "      <td>0.703084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.634500</td>\n",
              "      <td>0.704391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.600300</td>\n",
              "      <td>0.705917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>1.005000</td>\n",
              "      <td>0.707557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.604200</td>\n",
              "      <td>0.709208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.616300</td>\n",
              "      <td>0.710134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.621100</td>\n",
              "      <td>0.710645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.625600</td>\n",
              "      <td>0.710925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.582700</td>\n",
              "      <td>0.710860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.620100</td>\n",
              "      <td>0.711001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.600200</td>\n",
              "      <td>0.711142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/11/16 23:39:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run stylish-cod-396 at: https://5bb1-156-204-128-49.ngrok-free.app/#/experiments/1/runs/b735ae6d403e4f39bbbfc67860b77b58.\n",
            "2024/11/16 23:39:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://5bb1-156-204-128-49.ngrok-free.app/#/experiments/1.\n"
          ]
        }
      ],
      "source": [
        "# Configure and train the model with specified parameters\n",
        "model = configure_and_train_model(\n",
        "    r=128,                 # LoRA rank parameter\n",
        "    lora_alpha=16,        # Scaling factor for LoRA layers\n",
        "    lora_dropout=0.1,       # Set dropout to 0 (optimized)\n",
        "    use_gradient_checkpointing=\"none\",  # Memory optimization\n",
        "    random_state=3407,    # Seed for reproducibility\n",
        "    use_rslora=False,     # Disable rank-stabilized LoRA\n",
        "    loftq_config=None,    # Leave as None for default\n",
        "    train_dataset=train_data,\n",
        "    valid_dataset=valid_data,\n",
        "    batch_size=2,         # Set batch size for training\n",
        "    grad_accum_steps=16,   # Gradient accumulation steps\n",
        "    warmup_steps=10,       # Warmup steps for learning rate scheduler\n",
        "    max_steps=150,         # Total steps for quick testing\n",
        "    learning_rate=2e-3,   # Learning rate\n",
        "    weight_decay=0.01,    # Weight decay for optimizer\n",
        "    lr_scheduler_type=\"linear\",  # Learning rate scheduler type\n",
        "    output_dir=\"outputs\"  # Directory for saving results\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:39:42.223607Z",
          "iopub.status.busy": "2024-11-16T23:39:42.223137Z",
          "iopub.status.idle": "2024-11-16T23:39:42.235456Z",
          "shell.execute_reply": "2024-11-16T23:39:42.234481Z",
          "shell.execute_reply.started": "2024-11-16T23:39:42.223558Z"
        },
        "trusted": true,
        "id": "FPg2BCLZbLtk",
        "outputId": "d3a15fa4-90e6-444b-e6f6-be6093bd66c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peak reserved memory = 11.752 GB.\n",
            "Peak reserved memory for training = 9.447 GB.\n",
            "Peak reserved memory % of max memory = 79.723 %.\n",
            "Peak reserved memory for training % of max memory = 64.087 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS6fZ5MDbLtk"
      },
      "source": [
        "# inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:39:44.091497Z",
          "iopub.status.busy": "2024-11-16T23:39:44.090488Z",
          "iopub.status.idle": "2024-11-16T23:39:44.099152Z",
          "shell.execute_reply": "2024-11-16T23:39:44.098155Z",
          "shell.execute_reply.started": "2024-11-16T23:39:44.091450Z"
        },
        "trusted": true,
        "id": "j0C7H7R-bLtk"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"\"\"Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\n",
        "The best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\n",
        "\n",
        "### DESCRIPTION:\n",
        "{}\n",
        "\n",
        "### RESPONSE:\"\"\"\n",
        "\n",
        "\n",
        "def formatting_test_prompts_func(examples):\n",
        "    global tokenizer\n",
        "\n",
        "    inputs = examples[\"series_description\"]\n",
        "    texts = []\n",
        "    for input in  inputs:\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = test_prompt.format( input)\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:39:45.915866Z",
          "iopub.status.busy": "2024-11-16T23:39:45.914951Z",
          "iopub.status.idle": "2024-11-16T23:39:45.953419Z",
          "shell.execute_reply": "2024-11-16T23:39:45.952567Z",
          "shell.execute_reply.started": "2024-11-16T23:39:45.915821Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "c30e7ea569dc4843966cfa8d5ebcb3f7"
          ]
        },
        "id": "NEY74zCpbLtk",
        "outputId": "0e3b5fa2-d6ce-407d-c740-24f797d37007"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c30e7ea569dc4843966cfa8d5ebcb3f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['dataset_name', 'series_description', 'algorithm', 'hyperparameters', 'text'],\n",
              "    num_rows: 83\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset = test_data.map(formatting_test_prompts_func, batched = True)\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:39:47.782955Z",
          "iopub.status.busy": "2024-11-16T23:39:47.782155Z",
          "iopub.status.idle": "2024-11-16T23:39:50.870757Z",
          "shell.execute_reply": "2024-11-16T23:39:50.869464Z",
          "shell.execute_reply.started": "2024-11-16T23:39:47.782912Z"
        },
        "trusted": true,
        "id": "SVcnCJembLtl",
        "outputId": "20f4daf6-1115-4036-cb6b-b73476f25585"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<bos>Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\nThe best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\\n\\n### DESCRIPTION:\\nA univariate time-series dataset  consists of 48 samples with a missing values percentage of 0.0% imputed using FBProphet model and 0.0% detected outliers. The target series has a sampling rate of 1440 minutes, minimum value of -1.0577518085465023, maximum value of 0.5039540640878073, median value of -0.5925601507310199, mean value of -0.49076897775897593, and average standard deviation of 0.21373626312059368 for the 10 percentiles. The series is detected as non-stationary using dickey fuller testand it turns into a stationary series using first order differencing. The series has 6 significant lags observed using the partial autocorrelation function (pACF), the pACF values for these 6 lags ranges from 0.6091821079108509 for the lag number 1 to 0.3608509812237311 for the lag number 7. There exist 1 insignificant lags at these indices 4 between the first and the last significant ones. The series looks to be a multiplicative time-series with a logistic trend. There are no seasonality components detected in the seriesThe series exhibits a skewness value of 0.6670990699653278 and a kurtosis value of 0.47278337091329936. The Fractal dimension analysis yields a value of -0.4583557992511385, indicating a Complex and irregular time-series structure. The dataset is converted into a simple regression task by extracting the previously described features.\\n\\n### RESPONSE:<eos>XGBoostRegressor\\n\\n### DESCRIPTION:\\nA univariate time-series dataset  consists of 48 samples with a missing values percentage of 0.0% imputed using FBProphet model and 0.0% detected outliers. The target series has a sampling rate of 1440 minutes, minimum']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[test_dataset['text'][0]], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:39:55.973370Z",
          "iopub.status.busy": "2024-11-16T23:39:55.972500Z",
          "iopub.status.idle": "2024-11-16T23:39:56.392204Z",
          "shell.execute_reply": "2024-11-16T23:39:56.391376Z",
          "shell.execute_reply.started": "2024-11-16T23:39:55.973329Z"
        },
        "trusted": true,
        "id": "WweupPlLbLtl",
        "outputId": "cb1ff2a3-b6f3-44b9-b66f-459643646138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<bos>Below is a description for a time series data. Write a response that gives the name of the best fitting machine learning algorithm in one word without explanation.\\nThe best algorithm name should be one of this search space algorithms: AdaboostRegressor, ElasticNetRegressor,  ExtraTreesRegressor,  LassoRegressor,  LightgbmRegressor, SVR, GaussianProcessRegressor, RandomForestRegressor or  XGBoostRegressor.\\n\\n### DESCRIPTION:\\nA univariate time-series dataset  consists of 81 samples with a missing values percentage of 0.0% imputed using FBProphet model and 0.0% detected outliers. The target series has a sampling rate of 44640 minutes, minimum value of 3523.548387096774, maximum value of 6434.0, median value of 4978.387096774193, mean value of 4995.464271746457, and average standard deviation of 0.06947531163920204 for the 10 percentiles. The series is detected as stationary using dickey fuller test.The series has 7 significant lags observed using the partial autocorrelation function (pACF), the pACF values for these 7 lags ranges from 0.9375534431746181 for the lag number 1 to 0.2340147204900702 for the lag number 7.There are no insignificant lags in between the first and the last significant ones.The series looks to be a multiplicative time-series with a linear trend. There are 1 seasonality components detected in this series represented as sinusoidal waves with periods 27.The series exhibits a skewness value of 0.018459508790673103 and a kurtosis value of 1.0220489598846814. The Fractal dimension analysis yields a value of -0.07028093361365781, indicating a Complex and irregular time-series structure. The dataset is converted into a simple regression task by extracting the previously described features.\\n\\n### RESPONSE:<eos>ADABoostRegressor\\n\\n']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(\n",
        "[test_dataset['text'][1]], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 5, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:40:05.642647Z",
          "iopub.status.busy": "2024-11-16T23:40:05.642253Z",
          "iopub.status.idle": "2024-11-16T23:40:37.463840Z",
          "shell.execute_reply": "2024-11-16T23:40:37.463000Z",
          "shell.execute_reply.started": "2024-11-16T23:40:05.642611Z"
        },
        "trusted": true,
        "id": "fBG_iiHIbLtl"
      },
      "outputs": [],
      "source": [
        "test_responses=[]\n",
        "# get all test data inference result\n",
        "for test_prompt in test_dataset['text']:\n",
        "  inputs= tokenizer(\n",
        "  [test_prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 5, use_cache = True)\n",
        "  test_responses.append(tokenizer.batch_decode(outputs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:40:40.890317Z",
          "iopub.status.busy": "2024-11-16T23:40:40.889388Z",
          "iopub.status.idle": "2024-11-16T23:40:40.928344Z",
          "shell.execute_reply": "2024-11-16T23:40:40.927489Z",
          "shell.execute_reply.started": "2024-11-16T23:40:40.890274Z"
        },
        "trusted": true,
        "id": "Ns9nZ34YbLtl"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset to a Pandas DataFrame\n",
        "df = test_dataset.to_pandas()\n",
        "df['model_responses']= test_responses\n",
        "df.to_csv('test_model_result_unsloth.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:04.602329Z",
          "iopub.status.busy": "2024-11-16T23:51:04.601388Z",
          "iopub.status.idle": "2024-11-16T23:51:04.614642Z",
          "shell.execute_reply": "2024-11-16T23:51:04.613591Z",
          "shell.execute_reply.started": "2024-11-16T23:51:04.602283Z"
        },
        "trusted": true,
        "id": "r6Ot3eADbLtl",
        "outputId": "38d2ebf0-c17d-4566-f401-7845aeae368a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>ElasticNetRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>LassoRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>LightgbmRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>AdaboostRegressor',\n",
              " '<eos>XGBoostRegressor',\n",
              " '<eos>LightgbmRegressor']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = []\n",
        "for response in test_responses:\n",
        "    result = response[0].split('\\n\\n### RESPONSE:')[1].split('</s>')[0].strip()\n",
        "    result = result.replace('\\n', '').replace('.', '')  # Remove \\n and .\n",
        "    if result == '<eos>LIGHTgbmRegressor':\n",
        "        result = '<eos>LightgbmRegressor'\n",
        "    elif result == '<eos>ADABoostRegressor':\n",
        "        result = '<eos>AdaboostRegressor'\n",
        "\n",
        "    predictions.append(result)\n",
        "\n",
        "predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:11.159538Z",
          "iopub.status.busy": "2024-11-16T23:51:11.158837Z",
          "iopub.status.idle": "2024-11-16T23:51:11.168499Z",
          "shell.execute_reply": "2024-11-16T23:51:11.167527Z",
          "shell.execute_reply.started": "2024-11-16T23:51:11.159480Z"
        },
        "trusted": true,
        "id": "M0DDdVDEbLtl",
        "outputId": "e4acd75c-8c01-40aa-9583-96e71fd3e161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'ElasticNetRegressor', 'LightgbmRegressor', 'LightgbmRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'LassoRegressor', 'XGBoostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'LightgbmRegressor', 'XGBoostRegressor', 'AdaboostRegressor', 'XGBoostRegressor', 'LightgbmRegressor']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_model_name(prediction):\n",
        "    \"\"\"\n",
        "    Extract the model name from a prediction string using regex.\n",
        "    Corrects 'ADABoostRegressor' to 'AdaBoostRegressor'.\n",
        "    \"\"\"\n",
        "    # Adjust regex to match model names more precisely\n",
        "    match = re.search(r\"\\b[A-Za-z]+(?:[A-Za-z0-9]*)\\b\", prediction)\n",
        "    if match:\n",
        "        model_name = match.group()\n",
        "        return model_name\n",
        "    return None  # Return None if no match is found\n",
        "\n",
        "# Apply the extraction to all predictions\n",
        "extracted_predictions = [extract_model_name(pred.split('<eos>')[-1].strip()) for pred in predictions]\n",
        "\n",
        "# Print the extracted model names\n",
        "print(extracted_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:16.485670Z",
          "iopub.status.busy": "2024-11-16T23:51:16.484705Z",
          "iopub.status.idle": "2024-11-16T23:51:16.493170Z",
          "shell.execute_reply": "2024-11-16T23:51:16.492273Z",
          "shell.execute_reply.started": "2024-11-16T23:51:16.485627Z"
        },
        "trusted": true,
        "id": "2cLQZIo8bLtl",
        "outputId": "47ddafe6-5743-4c92-f3c2-3e0c7eb0d1df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(extracted_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:17.197507Z",
          "iopub.status.busy": "2024-11-16T23:51:17.196543Z",
          "iopub.status.idle": "2024-11-16T23:51:17.205302Z",
          "shell.execute_reply": "2024-11-16T23:51:17.204366Z",
          "shell.execute_reply.started": "2024-11-16T23:51:17.197467Z"
        },
        "trusted": true,
        "id": "FcafcOOybLtm",
        "outputId": "9909b871-5330-4faa-c6e2-2f2f5261a68b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actual_data= df['algorithm']\n",
        "len(actual_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:18.122118Z",
          "iopub.status.busy": "2024-11-16T23:51:18.120985Z",
          "iopub.status.idle": "2024-11-16T23:51:18.132077Z",
          "shell.execute_reply": "2024-11-16T23:51:18.130978Z",
          "shell.execute_reply.started": "2024-11-16T23:51:18.122064Z"
        },
        "trusted": true,
        "id": "FhGEZYPtbLtm",
        "outputId": "cf8b6a70-17db-4fbb-dd81-e87d4bcc43d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     GaussianProcessRegressor\n",
              "1     GaussianProcessRegressor\n",
              "2     GaussianProcessRegressor\n",
              "3          ElasticNetRegressor\n",
              "4             XGBoostRegressor\n",
              "                ...           \n",
              "78         ElasticNetRegressor\n",
              "79            XGBoostRegressor\n",
              "80           AdaboostRegressor\n",
              "81           AdaboostRegressor\n",
              "82              LassoRegressor\n",
              "Name: algorithm, Length: 83, dtype: object"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actual_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:18.907934Z",
          "iopub.status.busy": "2024-11-16T23:51:18.907477Z",
          "iopub.status.idle": "2024-11-16T23:51:18.921618Z",
          "shell.execute_reply": "2024-11-16T23:51:18.920454Z",
          "shell.execute_reply.started": "2024-11-16T23:51:18.907893Z"
        },
        "trusted": true,
        "id": "icYOr1-_bLtm",
        "outputId": "b0850bcb-eb8f-4cf9-c6a0-0262d133e33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score: 0.1566265060240964\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Assuming `predictions` and `actual_data` are lists or arrays of labels\n",
        "f1 = f1_score(actual_data, extracted_predictions, average='micro')  # Use 'macro' or 'micro' as needed\n",
        "\n",
        "\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:20.687032Z",
          "iopub.status.busy": "2024-11-16T23:51:20.686316Z",
          "iopub.status.idle": "2024-11-16T23:51:20.696764Z",
          "shell.execute_reply": "2024-11-16T23:51:20.695589Z",
          "shell.execute_reply.started": "2024-11-16T23:51:20.686989Z"
        },
        "trusted": true,
        "id": "-Id8K8hAbLtm",
        "outputId": "7b8e20e6-82c9-44b2-aeb2-fe30e1240d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.1566265060240964\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(actual_data, extracted_predictions)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5tHVcAobLtm"
      },
      "source": [
        "# save tuned model\n",
        "\n",
        "To save the final model as LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:26.901075Z",
          "iopub.status.busy": "2024-11-16T23:51:26.900203Z",
          "iopub.status.idle": "2024-11-16T23:51:29.053300Z",
          "shell.execute_reply": "2024-11-16T23:51:29.052269Z",
          "shell.execute_reply.started": "2024-11-16T23:51:26.901033Z"
        },
        "trusted": true,
        "id": "3_cJYHn2bLtn",
        "outputId": "552d7426-2f01-4393-f8fb-a8d4eef7d4d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.model',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Local saving\n",
        "\n",
        "model.save_pretrained(\"lora_model\")\n",
        "\n",
        "tokenizer.save_pretrained(\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:29.056114Z",
          "iopub.status.busy": "2024-11-16T23:51:29.055362Z",
          "iopub.status.idle": "2024-11-16T23:51:36.650222Z",
          "shell.execute_reply": "2024-11-16T23:51:36.649447Z",
          "shell.execute_reply.started": "2024-11-16T23:51:29.056066Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "319b736658444b74ab7f75433b7e61f4",
            "d7b03f1fa0b94236b0c2eb992f8cf792",
            "4109eae41c46464190fb3a259268f81d"
          ]
        },
        "id": "mrXkbTxYbLtn",
        "outputId": "7e4454b5-0943-4318-d8fc-abf3d38f3fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "319b736658444b74ab7f75433b7e61f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/576 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7b03f1fa0b94236b0c2eb992f8cf792",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4109eae41c46464190fb3a259268f81d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/628M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/RanaHossny213/gamma_tuned-2b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ],
      "source": [
        "# Online saving on HF\n",
        "from huggingface_hub import login\n",
        "\n",
        "new_model_adabtor= \"RanaHossny213/gamma_tuned-2b\"\n",
        "login(token=\"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\")  # Use your Hugging Face token\n",
        "\n",
        "# Push the model and tokenizer to the Hugging Face hub\n",
        "model.push_to_hub(new_model_adabtor)\n",
        "tokenizer.push_to_hub(new_model_adabtor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:51:36.652288Z",
          "iopub.status.busy": "2024-11-16T23:51:36.651961Z",
          "iopub.status.idle": "2024-11-16T23:52:19.097932Z",
          "shell.execute_reply": "2024-11-16T23:52:19.097006Z",
          "shell.execute_reply.started": "2024-11-16T23:51:36.652252Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "cafc8b0e72ea4a4fb8ec15caffa26f9b",
            "95f043a6890844cb8bc262ed20fa18a2",
            "c9165d6e5f6d4f6b8bf1bf996d84e048"
          ]
        },
        "id": "4JYv_uQ9bLtn",
        "outputId": "a9af862d-921a-4b71-e72f-33212660becd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
            "This might take 5 minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 10 minutes for Llama-7b... Done.\n",
            "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
            "This might take 5 minutes...\n",
            "Done.\n",
            "Unsloth: Saving 4bit Bitsandbytes model. Please wait...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cafc8b0e72ea4a4fb8ec15caffa26f9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/582 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95f043a6890844cb8bc262ed20fa18a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9165d6e5f6d4f6b8bf1bf996d84e048",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.07G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged_4bit model to https://huggingface.co/model\n"
          ]
        }
      ],
      "source": [
        "# Save and Merge to 4bit\n",
        "\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\",token = \"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\")\n",
        "\n",
        "model.push_to_hub_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\", token = \"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-16T23:52:19.129480Z",
          "iopub.status.busy": "2024-11-16T23:52:19.129061Z",
          "iopub.status.idle": "2024-11-16T23:52:27.053173Z",
          "shell.execute_reply": "2024-11-16T23:52:27.052118Z",
          "shell.execute_reply.started": "2024-11-16T23:52:19.129434Z"
        },
        "trusted": true,
        "id": "rhO8OFGxbLtn",
        "outputId": "3cbfe8dd-86c5-4201-94f9-142bef991dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... Done.\n",
            "Unsloth: Saving LoRA adapters. Please wait...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved lora model to https://huggingface.co/model\n"
          ]
        }
      ],
      "source": [
        "# Save just LoRA adapters\n",
        "\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",token = \"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\")\n",
        "\n",
        "model.push_to_hub_merged(\"model\", tokenizer, save_method = \"lora\", token = \"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmOoPUlTbLto"
      },
      "source": [
        "# i couldnot store gguf becouse the storage of kaggle so i upload the notebook on colab and load the tuned model to generate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432,
          "referenced_widgets": [
            "a27f337538bc45b7b9baccb4d4fc2113",
            "95d54cbf13fd4d84bbd6d2360c20f711",
            "7e072d76cf5c42ff8649831266c92041",
            "309ef7064a2749febc2e8d49de165dcc",
            "696fed4fa9e5408d944a5d7797c10908",
            "816738ce4f564304be818a8d2285186f",
            "af234091bda44f06a237c3d2224ee6dd",
            "54b899501e1942a2a233cc81525f235d",
            "1d77d5b91eb44f4eb952323e3cc74cb6",
            "397fe5665a704b499a2cd4147f3e3740",
            "c7cc3172bfc64ba1bb72ce4faf8c228a",
            "ee8d9192eb6d4965bc3c0e5466145632",
            "b3944c0df7e24229938444a9c15869d0",
            "abb3f49a8bec46eeace0f8b3b0006991",
            "9088c06172d54bee9bf1d883e282ba45",
            "4d82611dc1994bbaa71167a046688118",
            "fb9df12f16b444efb026d0b852f9a552",
            "68e14809e2bb463fba35a35c433ffdf1",
            "c947fb2ccd3e4955aa483cc6e7f17ca7",
            "96aae021638e4165bcfc2618234def56",
            "c26b8fdbf1664c8f9fb2562580830d65",
            "95f6254603e8493782b6b621e1a091a6",
            "8efe90502f514aafb393c527ead215a6",
            "c1c6705f07764242b2867d255989fae2",
            "8c6f8fe80dca475d9eb831ceb60e854e",
            "cb38da3856434fceae45519490f55180",
            "f6496c0bc99f49868daa957e23d1f804",
            "c65126070f894ffb89b4b6ed52274056",
            "4d8ca770d2f64378afed7026d2e13a89",
            "ef7e395812d941568f14888419d396ec",
            "0dd8892e859142579f9f69e6d096ed6d",
            "735d4dcb8dd9444ca710f7f7ecee186b",
            "985caee3b8f64a009b3f285170af6291",
            "4739dbe5e69742cebf2e05a6df1abac9",
            "9b6819a4349245e9bc357d4e80819236",
            "b2ba048fd7ed4015af6f747987a9749d",
            "540aa8d6cb354825987821fe9e6c3410",
            "9d16f28826f4427c9f115b8e5049a644",
            "0245293ee44b48dfbb2d7e1209dd2b8d",
            "e5f9d4307cbe48149a917ac9c99d706d",
            "babd0eb530f842f49063b52dc1125aff",
            "1e8f7a7240e54f39b5c9332e2f20e7b0",
            "9c0c5b55cb3148ecaeab26c29cef4e18",
            "138d56b5b28b426c843f40398deca9a6",
            "ba33b397e4f54dd9b218e88fb253e230",
            "58c91c266bf64629bb12cf4279ee66cb",
            "bdda08608f4a447f97fd98def99577ab",
            "46055cd24fb9407bac5b355b497715e7",
            "cdecee9d2c0e4a758cac4233b8d64f49",
            "8fa61f27ec224cc184c7a6c6cd909679",
            "42e946072ce84140b8585d4bd8b9f35d",
            "bd60235441704b0ea5e6d9c371976edb",
            "931c13dd84374e68a3b4806f63646a30",
            "3695f180533c4cd0b05d8f0a889e4155",
            "ee92bad9ead94f9c9078b217a126e64d",
            "d3e22c235cbf423a86314b9094f67f9d",
            "1fe5d8970fe54768bdc03366a68f4245",
            "3d5b180e05b74d45b41ce5d6c2b56450",
            "5f6da0490c864c6680cc4aa35f4bd5b6",
            "b2141f2984e84eb38486d7f5e9027af8",
            "a5b82816ed0844699849d6361151b3dc",
            "6765413bef994fefb334e4609907beaf",
            "19e605d8b3664767993d8d826e29574b",
            "a28a4230f4814c5b8b4aa167778ee144",
            "58631dd8fd0748bdb870bd916448fa87",
            "c674b894ad6d4bf68c37eb9f57b9b6e5",
            "0a979c61c7c54f03b2f9fbc8a93c8e44",
            "a8456a0856fe4524b3eac82c3def6e1c",
            "611509652efc43cd986d09c5c25dcac7",
            "688df2128d2c439e82e7df9368eb2fbe",
            "fd7976e7dd61498b98b2f5c55e1cbfb4",
            "e430fbf4716242ad804d802ea632af4b",
            "ffff483fee0948cf81d894eedc26206b",
            "50093e75a39a4be5825b7953db5df9e8",
            "39a8a60bb6a94176a5ec4b84385656c7",
            "325130712fd240b7bc277460f0543c6b",
            "db6d5073dbaf42189f14b734613ab0c3"
          ]
        },
        "id": "Ll5aOQ1sbLto",
        "outputId": "f15940b4-ad95-4eb7-c346-b8b8d4812e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.11.7: Fast Gemma patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 7.5. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a27f337538bc45b7b9baccb4d4fc2113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee8d9192eb6d4965bc3c0e5466145632"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8efe90502f514aafb393c527ead215a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4739dbe5e69742cebf2e05a6df1abac9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba33b397e4f54dd9b218e88fb253e230"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3e22c235cbf423a86314b9094f67f9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/628M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a979c61c7c54f03b2f9fbc8a93c8e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.11.7 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "max_seq_length = 2048# Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"RanaHossny213/gamma_tuned-2b\",\n",
        "    max_seq_length # Add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "= max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(\"gamma-ft-gguf\", tokenizer, quantization_method = \"q4_k_m\", token=\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "47fd2adfba304b7fa635ed25c6d84c25",
            "2d2d6ca437a045b6afaf9547c580606c",
            "82c7b0b0e24541de88807afa5c628d0f",
            "e81fde3d327343fe9ef66360d16dffae",
            "42d5adee62be4febb890f755a363c37f",
            "a08bbd3869694cf5a5021849f11ee943",
            "8d07a109cf134cf98ee0ad3b1ce62b8c",
            "f60b31d2427444ec8b3665cf62c7f696",
            "2cce294349ae43dab576e8255f66a1a9",
            "2e13eb03b195413b8e7761c74b8438da",
            "4fca7b6918fa4d6aa0ce9088a441dd81",
            "5c57bdb5699d4fc89c8f37d4125d024d",
            "4c4e303cdcb946e49f7b3e199513d329",
            "61ef8598981f4c48b8105ebc0660442d",
            "7474583927c64edbbeef90f5e88c7924",
            "b9802a3e6471491ba40eddb318cea466",
            "f900564a12fb443c9bb4739a1a60b79c",
            "9145ec44b14e441bb62a9ba5b0556e48",
            "48662b843dda4bc3887fa7531bc762a5",
            "c5e7b5b1e3f34377b526bc1aa0aa7db2",
            "62b21ae684d240aebb5eb56d9d43a928",
            "23189f85eb584242ad349f5e914be000"
          ]
        },
        "id": "SPCUufqIe_sm",
        "outputId": "83f8f54f-d4fa-417f-a6de-f3cd8643c46d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 5.0G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 7.08 out of 12.67 RAM for saving.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:01<00:00, 14.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
            "Unsloth: Saving gamma-ft-gguf/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving gamma-ft-gguf/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting gemma model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
            "Unsloth: [1] Converting model at gamma-ft-gguf into f16 GGUF format.\n",
            "The output location will be /content/gamma-ft-gguf/unsloth.F16.gguf\n",
            "This will take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: gamma-ft-gguf\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 256000}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 16384}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {16384, 2048}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 2\n",
            "INFO:gguf.vocab:Setting special token type eos to 1\n",
            "INFO:gguf.vocab:Setting special token type unk to 3\n",
            "INFO:gguf.vocab:Setting special token type pad to 0\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "WARNING:gguf.vocab:No handler for special token type prefix with id 67 - skipping\n",
            "WARNING:gguf.vocab:No handler for special token type suffix with id 69 - skipping\n",
            "WARNING:gguf.vocab:No handler for special token type middle with id 68 - skipping\n",
            "WARNING:gguf.vocab:No handler for special token type fsep with id 70 - skipping\n",
            "INFO:gguf.vocab:Setting special token type eot to 107\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/gamma-ft-gguf/unsloth.F16.gguf: n_tensors = 164, total_size = 5.0G\n",
            "Writing: 100%|██████████| 5.01G/5.01G [01:02<00:00, 80.2Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/gamma-ft-gguf/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/gamma-ft-gguf/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
            "main: build = 4119 (ce2e59ba)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/gamma-ft-gguf/unsloth.F16.gguf' to '/content/gamma-ft-gguf/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 164 tensors from /content/gamma-ft-gguf/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2b\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.basename str              = gemma\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
            "llama_model_loader: - kv   6:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   7:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   8:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   9:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv  10:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv  11:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv  12:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  13:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  14:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eot_token_id u32              = 107\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   37 tensors\n",
            "llama_model_loader: - type  f16:  127 tensors\n",
            "[   1/ 164]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 164]                    token_embd.weight - [ 2048, 256000,     1,     1], type =    f16, converting to q6_K .. size =  1000.00 MiB ->   410.16 MiB\n",
            "[   3/ 164]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[   4/ 164]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   5/ 164]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   6/ 164]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 164]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[   8/ 164]                blk.0.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[   9/ 164]                blk.0.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  10/ 164]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  11/ 164]                  blk.0.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  12/ 164]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  13/ 164]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  14/ 164]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  15/ 164]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  16/ 164]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  17/ 164]                blk.1.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[  18/ 164]                blk.1.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  19/ 164]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  20/ 164]                  blk.1.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  21/ 164]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  22/ 164]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  23/ 164]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  24/ 164]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  25/ 164]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  26/ 164]                blk.2.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  27/ 164]                blk.2.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  28/ 164]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  29/ 164]                  blk.2.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  30/ 164]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  31/ 164]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  32/ 164]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  33/ 164]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  34/ 164]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  35/ 164]                blk.3.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  36/ 164]                blk.3.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  37/ 164]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  38/ 164]                  blk.3.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  39/ 164]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  40/ 164]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  41/ 164]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  42/ 164]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  43/ 164]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  44/ 164]                blk.4.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[  45/ 164]                blk.4.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  46/ 164]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  47/ 164]                  blk.4.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  48/ 164]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  49/ 164]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  50/ 164]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  51/ 164]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  52/ 164]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  53/ 164]                blk.5.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  54/ 164]                blk.5.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  55/ 164]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  56/ 164]                  blk.5.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  57/ 164]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  58/ 164]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  59/ 164]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  60/ 164]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  61/ 164]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  62/ 164]                blk.6.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  63/ 164]                blk.6.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  64/ 164]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  65/ 164]                  blk.6.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  66/ 164]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  67/ 164]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  68/ 164]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  69/ 164]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  70/ 164]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  71/ 164]                blk.7.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[  72/ 164]                blk.7.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  73/ 164]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  74/ 164]                  blk.7.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  75/ 164]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  76/ 164]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  77/ 164]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  78/ 164]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  79/ 164]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  80/ 164]                blk.8.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  81/ 164]                blk.8.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  82/ 164]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  83/ 164]                  blk.8.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  84/ 164]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  85/ 164]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  86/ 164]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  87/ 164]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  88/ 164]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  89/ 164]                blk.9.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  90/ 164]                blk.9.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  91/ 164]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  92/ 164]                  blk.9.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[  93/ 164]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[  94/ 164]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  95/ 164]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  96/ 164]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  97/ 164]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[  98/ 164]               blk.10.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[  99/ 164]               blk.10.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 100/ 164]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 101/ 164]                 blk.10.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 102/ 164]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 103/ 164]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 104/ 164]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 105/ 164]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 106/ 164]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 107/ 164]               blk.11.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 108/ 164]               blk.11.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 109/ 164]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 110/ 164]                 blk.11.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 111/ 164]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 112/ 164]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 113/ 164]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 114/ 164]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 115/ 164]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 116/ 164]               blk.12.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 117/ 164]               blk.12.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 118/ 164]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 119/ 164]                 blk.12.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 120/ 164]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 121/ 164]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 122/ 164]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 123/ 164]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 124/ 164]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 125/ 164]               blk.13.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[ 126/ 164]               blk.13.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 127/ 164]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 128/ 164]                 blk.13.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 129/ 164]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 130/ 164]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 131/ 164]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 132/ 164]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 133/ 164]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 134/ 164]               blk.14.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 135/ 164]               blk.14.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 136/ 164]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 137/ 164]                 blk.14.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 138/ 164]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 139/ 164]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 140/ 164]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 141/ 164]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 142/ 164]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 143/ 164]               blk.15.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[ 144/ 164]               blk.15.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 145/ 164]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 146/ 164]                 blk.15.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 147/ 164]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 148/ 164]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 149/ 164]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 150/ 164]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 151/ 164]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 152/ 164]               blk.16.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[ 153/ 164]               blk.16.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 154/ 164]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 155/ 164]                 blk.16.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 156/ 164]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n",
            "[ 157/ 164]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 158/ 164]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 159/ 164]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 160/ 164]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n",
            "[ 161/ 164]               blk.17.ffn_down.weight - [16384,  2048,     1,     1], type =    f16, converting to q6_K .. size =    64.00 MiB ->    26.25 MiB\n",
            "[ 162/ 164]               blk.17.ffn_gate.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "[ 163/ 164]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 164/ 164]                 blk.17.ffn_up.weight - [ 2048, 16384,     1,     1], type =    f16, converting to q4_K .. size =    64.00 MiB ->    18.00 MiB\n",
            "llama_model_quantize_internal: model size  =  4780.29 MB\n",
            "llama_model_quantize_internal: quant size  =  1548.98 MB\n",
            "\n",
            "main: quantize time = 285118.81 ms\n",
            "main:    total time = 285118.81 ms\n",
            "Unsloth: Conversion completed! Output location: /content/gamma-ft-gguf/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47fd2adfba304b7fa635ed25c6d84c25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c57bdb5699d4fc89c8f37d4125d024d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/RanaHossny213/gamma-ft-gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\", token = \"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "3cbd3f2970a7447c99642d85bc0202c2",
            "6a898b2f8ff24bce86b358bb07449e4f",
            "9c8276f808374890aa0d9886c8d42077",
            "66a7f9f014db48409c37ba0e5dce3899",
            "6b6d7c4868474389bb79c3fd87256d93",
            "c8e27e57c4164872af97f39c351fc98a",
            "ce788becd95b4815a57e40ec6874158a",
            "cc16d09d39de41fab6157b4e254fde0f",
            "508f9d4273ef4cfe8f95b12f5708ddc1",
            "8c51ae66864d4ccd8a368a9f38f4f468",
            "19421384e1314301bed2f6dba6cba42e",
            "7dfcef4955a94ea0aff24e14c87db175",
            "3e7662ea7c3e48c8a5ee1726daa91650",
            "c054c9d6cf7d4614ac52bace7b4ac7d7",
            "dd9507ff06b446ba9c36226607eddb44",
            "acb279cf047140d2bc2d527d60044fc8",
            "5add3f49a9294b48b5b420b63e917c41",
            "d74aa61789314c0cbe502f8242a30599",
            "1c77ea82e010462ea3bee705316fb952",
            "d4b28d808c5a40588b8322eb9edc3c2b",
            "f5f70703a7ef43eaa132e6dbd4850bda",
            "eb14ecf14c2e4472ad783bf141299b30",
            "cb08627ca7e743a3949d92df28d1b899",
            "1b3524633f314610b1ce534481a4b4e2",
            "d656053c023e4f4d9aeb712762afe879",
            "f8fe738b08a34dc8bebe2a887a44b523",
            "4687a37ed75743d88436cfcab8ee3f5c",
            "d283939e136043a2a9e66335e6185333",
            "68b16221773e4019bc7cf757944dbf18",
            "58fe002ce02e45d7ab6be6dd1e193cf8",
            "a364325f3aa44a24a70340e3f60a035b",
            "3f1e2064c58d47a7b80fdbef46c9029c",
            "f441ccb5fe4b4a5f9e6ec2842441cc70"
          ]
        },
        "id": "y9KmDOHle_4V",
        "outputId": "c68703f1-c5fd-49a0-bbd2-a9a9854117bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
            "This might take 5 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Unsloth: Saving 4bit Bitsandbytes model. Please wait...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/582 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cbd3f2970a7447c99642d85bc0202c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dfcef4955a94ea0aff24e14c87db175"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb08627ca7e743a3949d92df28d1b899"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved merged_4bit model to https://huggingface.co/model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_C4z_QTqgBs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6060942,
          "sourceId": 9872956,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6087213,
          "sourceId": 9907752,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a27f337538bc45b7b9baccb4d4fc2113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95d54cbf13fd4d84bbd6d2360c20f711",
              "IPY_MODEL_7e072d76cf5c42ff8649831266c92041",
              "IPY_MODEL_309ef7064a2749febc2e8d49de165dcc"
            ],
            "layout": "IPY_MODEL_696fed4fa9e5408d944a5d7797c10908"
          }
        },
        "95d54cbf13fd4d84bbd6d2360c20f711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_816738ce4f564304be818a8d2285186f",
            "placeholder": "​",
            "style": "IPY_MODEL_af234091bda44f06a237c3d2224ee6dd",
            "value": "model.safetensors: 100%"
          }
        },
        "7e072d76cf5c42ff8649831266c92041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54b899501e1942a2a233cc81525f235d",
            "max": 5012363856,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d77d5b91eb44f4eb952323e3cc74cb6",
            "value": 5012363378
          }
        },
        "309ef7064a2749febc2e8d49de165dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_397fe5665a704b499a2cd4147f3e3740",
            "placeholder": "​",
            "style": "IPY_MODEL_c7cc3172bfc64ba1bb72ce4faf8c228a",
            "value": " 5.01G/5.01G [00:41&lt;00:00, 687MB/s]"
          }
        },
        "696fed4fa9e5408d944a5d7797c10908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816738ce4f564304be818a8d2285186f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af234091bda44f06a237c3d2224ee6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b899501e1942a2a233cc81525f235d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d77d5b91eb44f4eb952323e3cc74cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "397fe5665a704b499a2cd4147f3e3740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7cc3172bfc64ba1bb72ce4faf8c228a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee8d9192eb6d4965bc3c0e5466145632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3944c0df7e24229938444a9c15869d0",
              "IPY_MODEL_abb3f49a8bec46eeace0f8b3b0006991",
              "IPY_MODEL_9088c06172d54bee9bf1d883e282ba45"
            ],
            "layout": "IPY_MODEL_4d82611dc1994bbaa71167a046688118"
          }
        },
        "b3944c0df7e24229938444a9c15869d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb9df12f16b444efb026d0b852f9a552",
            "placeholder": "​",
            "style": "IPY_MODEL_68e14809e2bb463fba35a35c433ffdf1",
            "value": "generation_config.json: 100%"
          }
        },
        "abb3f49a8bec46eeace0f8b3b0006991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c947fb2ccd3e4955aa483cc6e7f17ca7",
            "max": 154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96aae021638e4165bcfc2618234def56",
            "value": 154
          }
        },
        "9088c06172d54bee9bf1d883e282ba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c26b8fdbf1664c8f9fb2562580830d65",
            "placeholder": "​",
            "style": "IPY_MODEL_95f6254603e8493782b6b621e1a091a6",
            "value": " 154/154 [00:00&lt;00:00, 10.7kB/s]"
          }
        },
        "4d82611dc1994bbaa71167a046688118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb9df12f16b444efb026d0b852f9a552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68e14809e2bb463fba35a35c433ffdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c947fb2ccd3e4955aa483cc6e7f17ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96aae021638e4165bcfc2618234def56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c26b8fdbf1664c8f9fb2562580830d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f6254603e8493782b6b621e1a091a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8efe90502f514aafb393c527ead215a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1c6705f07764242b2867d255989fae2",
              "IPY_MODEL_8c6f8fe80dca475d9eb831ceb60e854e",
              "IPY_MODEL_cb38da3856434fceae45519490f55180"
            ],
            "layout": "IPY_MODEL_f6496c0bc99f49868daa957e23d1f804"
          }
        },
        "c1c6705f07764242b2867d255989fae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c65126070f894ffb89b4b6ed52274056",
            "placeholder": "​",
            "style": "IPY_MODEL_4d8ca770d2f64378afed7026d2e13a89",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8c6f8fe80dca475d9eb831ceb60e854e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef7e395812d941568f14888419d396ec",
            "max": 40017,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dd8892e859142579f9f69e6d096ed6d",
            "value": 40017
          }
        },
        "cb38da3856434fceae45519490f55180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_735d4dcb8dd9444ca710f7f7ecee186b",
            "placeholder": "​",
            "style": "IPY_MODEL_985caee3b8f64a009b3f285170af6291",
            "value": " 40.0k/40.0k [00:00&lt;00:00, 2.19MB/s]"
          }
        },
        "f6496c0bc99f49868daa957e23d1f804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c65126070f894ffb89b4b6ed52274056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8ca770d2f64378afed7026d2e13a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef7e395812d941568f14888419d396ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd8892e859142579f9f69e6d096ed6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "735d4dcb8dd9444ca710f7f7ecee186b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "985caee3b8f64a009b3f285170af6291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4739dbe5e69742cebf2e05a6df1abac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b6819a4349245e9bc357d4e80819236",
              "IPY_MODEL_b2ba048fd7ed4015af6f747987a9749d",
              "IPY_MODEL_540aa8d6cb354825987821fe9e6c3410"
            ],
            "layout": "IPY_MODEL_9d16f28826f4427c9f115b8e5049a644"
          }
        },
        "9b6819a4349245e9bc357d4e80819236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0245293ee44b48dfbb2d7e1209dd2b8d",
            "placeholder": "​",
            "style": "IPY_MODEL_e5f9d4307cbe48149a917ac9c99d706d",
            "value": "tokenizer.model: 100%"
          }
        },
        "b2ba048fd7ed4015af6f747987a9749d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_babd0eb530f842f49063b52dc1125aff",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e8f7a7240e54f39b5c9332e2f20e7b0",
            "value": 4241003
          }
        },
        "540aa8d6cb354825987821fe9e6c3410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c0c5b55cb3148ecaeab26c29cef4e18",
            "placeholder": "​",
            "style": "IPY_MODEL_138d56b5b28b426c843f40398deca9a6",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 42.5MB/s]"
          }
        },
        "9d16f28826f4427c9f115b8e5049a644": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0245293ee44b48dfbb2d7e1209dd2b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f9d4307cbe48149a917ac9c99d706d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "babd0eb530f842f49063b52dc1125aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e8f7a7240e54f39b5c9332e2f20e7b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c0c5b55cb3148ecaeab26c29cef4e18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "138d56b5b28b426c843f40398deca9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba33b397e4f54dd9b218e88fb253e230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58c91c266bf64629bb12cf4279ee66cb",
              "IPY_MODEL_bdda08608f4a447f97fd98def99577ab",
              "IPY_MODEL_46055cd24fb9407bac5b355b497715e7"
            ],
            "layout": "IPY_MODEL_cdecee9d2c0e4a758cac4233b8d64f49"
          }
        },
        "58c91c266bf64629bb12cf4279ee66cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fa61f27ec224cc184c7a6c6cd909679",
            "placeholder": "​",
            "style": "IPY_MODEL_42e946072ce84140b8585d4bd8b9f35d",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "bdda08608f4a447f97fd98def99577ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd60235441704b0ea5e6d9c371976edb",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_931c13dd84374e68a3b4806f63646a30",
            "value": 636
          }
        },
        "46055cd24fb9407bac5b355b497715e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3695f180533c4cd0b05d8f0a889e4155",
            "placeholder": "​",
            "style": "IPY_MODEL_ee92bad9ead94f9c9078b217a126e64d",
            "value": " 636/636 [00:00&lt;00:00, 43.2kB/s]"
          }
        },
        "cdecee9d2c0e4a758cac4233b8d64f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa61f27ec224cc184c7a6c6cd909679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42e946072ce84140b8585d4bd8b9f35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd60235441704b0ea5e6d9c371976edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931c13dd84374e68a3b4806f63646a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3695f180533c4cd0b05d8f0a889e4155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee92bad9ead94f9c9078b217a126e64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3e22c235cbf423a86314b9094f67f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fe5d8970fe54768bdc03366a68f4245",
              "IPY_MODEL_3d5b180e05b74d45b41ce5d6c2b56450",
              "IPY_MODEL_5f6da0490c864c6680cc4aa35f4bd5b6"
            ],
            "layout": "IPY_MODEL_b2141f2984e84eb38486d7f5e9027af8"
          }
        },
        "1fe5d8970fe54768bdc03366a68f4245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5b82816ed0844699849d6361151b3dc",
            "placeholder": "​",
            "style": "IPY_MODEL_6765413bef994fefb334e4609907beaf",
            "value": "tokenizer.json: 100%"
          }
        },
        "3d5b180e05b74d45b41ce5d6c2b56450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e605d8b3664767993d8d826e29574b",
            "max": 17518525,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a28a4230f4814c5b8b4aa167778ee144",
            "value": 17518525
          }
        },
        "5f6da0490c864c6680cc4aa35f4bd5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58631dd8fd0748bdb870bd916448fa87",
            "placeholder": "​",
            "style": "IPY_MODEL_c674b894ad6d4bf68c37eb9f57b9b6e5",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 41.8MB/s]"
          }
        },
        "b2141f2984e84eb38486d7f5e9027af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5b82816ed0844699849d6361151b3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6765413bef994fefb334e4609907beaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19e605d8b3664767993d8d826e29574b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a28a4230f4814c5b8b4aa167778ee144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58631dd8fd0748bdb870bd916448fa87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c674b894ad6d4bf68c37eb9f57b9b6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a979c61c7c54f03b2f9fbc8a93c8e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8456a0856fe4524b3eac82c3def6e1c",
              "IPY_MODEL_611509652efc43cd986d09c5c25dcac7",
              "IPY_MODEL_688df2128d2c439e82e7df9368eb2fbe"
            ],
            "layout": "IPY_MODEL_fd7976e7dd61498b98b2f5c55e1cbfb4"
          }
        },
        "a8456a0856fe4524b3eac82c3def6e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e430fbf4716242ad804d802ea632af4b",
            "placeholder": "​",
            "style": "IPY_MODEL_ffff483fee0948cf81d894eedc26206b",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "611509652efc43cd986d09c5c25dcac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50093e75a39a4be5825b7953db5df9e8",
            "max": 627606952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39a8a60bb6a94176a5ec4b84385656c7",
            "value": 627606893
          }
        },
        "688df2128d2c439e82e7df9368eb2fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325130712fd240b7bc277460f0543c6b",
            "placeholder": "​",
            "style": "IPY_MODEL_db6d5073dbaf42189f14b734613ab0c3",
            "value": " 628M/628M [00:05&lt;00:00, 203MB/s]"
          }
        },
        "fd7976e7dd61498b98b2f5c55e1cbfb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e430fbf4716242ad804d802ea632af4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffff483fee0948cf81d894eedc26206b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50093e75a39a4be5825b7953db5df9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a8a60bb6a94176a5ec4b84385656c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "325130712fd240b7bc277460f0543c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db6d5073dbaf42189f14b734613ab0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47fd2adfba304b7fa635ed25c6d84c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d2d6ca437a045b6afaf9547c580606c",
              "IPY_MODEL_82c7b0b0e24541de88807afa5c628d0f",
              "IPY_MODEL_e81fde3d327343fe9ef66360d16dffae"
            ],
            "layout": "IPY_MODEL_42d5adee62be4febb890f755a363c37f"
          }
        },
        "2d2d6ca437a045b6afaf9547c580606c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08bbd3869694cf5a5021849f11ee943",
            "placeholder": "​",
            "style": "IPY_MODEL_8d07a109cf134cf98ee0ad3b1ce62b8c",
            "value": "100%"
          }
        },
        "82c7b0b0e24541de88807afa5c628d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f60b31d2427444ec8b3665cf62c7f696",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cce294349ae43dab576e8255f66a1a9",
            "value": 1
          }
        },
        "e81fde3d327343fe9ef66360d16dffae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e13eb03b195413b8e7761c74b8438da",
            "placeholder": "​",
            "style": "IPY_MODEL_4fca7b6918fa4d6aa0ce9088a441dd81",
            "value": " 1/1 [00:18&lt;00:00, 18.87s/it]"
          }
        },
        "42d5adee62be4febb890f755a363c37f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08bbd3869694cf5a5021849f11ee943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d07a109cf134cf98ee0ad3b1ce62b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f60b31d2427444ec8b3665cf62c7f696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cce294349ae43dab576e8255f66a1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e13eb03b195413b8e7761c74b8438da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fca7b6918fa4d6aa0ce9088a441dd81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c57bdb5699d4fc89c8f37d4125d024d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c4e303cdcb946e49f7b3e199513d329",
              "IPY_MODEL_61ef8598981f4c48b8105ebc0660442d",
              "IPY_MODEL_7474583927c64edbbeef90f5e88c7924"
            ],
            "layout": "IPY_MODEL_b9802a3e6471491ba40eddb318cea466"
          }
        },
        "4c4e303cdcb946e49f7b3e199513d329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f900564a12fb443c9bb4739a1a60b79c",
            "placeholder": "​",
            "style": "IPY_MODEL_9145ec44b14e441bb62a9ba5b0556e48",
            "value": "unsloth.Q4_K_M.gguf: "
          }
        },
        "61ef8598981f4c48b8105ebc0660442d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48662b843dda4bc3887fa7531bc762a5",
            "max": 1630261696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5e7b5b1e3f34377b526bc1aa0aa7db2",
            "value": 1630261696
          }
        },
        "7474583927c64edbbeef90f5e88c7924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b21ae684d240aebb5eb56d9d43a928",
            "placeholder": "​",
            "style": "IPY_MODEL_23189f85eb584242ad349f5e914be000",
            "value": " 1.63G/? [00:18&lt;00:00, 793MB/s]"
          }
        },
        "b9802a3e6471491ba40eddb318cea466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f900564a12fb443c9bb4739a1a60b79c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9145ec44b14e441bb62a9ba5b0556e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48662b843dda4bc3887fa7531bc762a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e7b5b1e3f34377b526bc1aa0aa7db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62b21ae684d240aebb5eb56d9d43a928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23189f85eb584242ad349f5e914be000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cbd3f2970a7447c99642d85bc0202c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a898b2f8ff24bce86b358bb07449e4f",
              "IPY_MODEL_9c8276f808374890aa0d9886c8d42077",
              "IPY_MODEL_66a7f9f014db48409c37ba0e5dce3899"
            ],
            "layout": "IPY_MODEL_6b6d7c4868474389bb79c3fd87256d93"
          }
        },
        "6a898b2f8ff24bce86b358bb07449e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8e27e57c4164872af97f39c351fc98a",
            "placeholder": "​",
            "style": "IPY_MODEL_ce788becd95b4815a57e40ec6874158a",
            "value": "README.md: 100%"
          }
        },
        "9c8276f808374890aa0d9886c8d42077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc16d09d39de41fab6157b4e254fde0f",
            "max": 582,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_508f9d4273ef4cfe8f95b12f5708ddc1",
            "value": 582
          }
        },
        "66a7f9f014db48409c37ba0e5dce3899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c51ae66864d4ccd8a368a9f38f4f468",
            "placeholder": "​",
            "style": "IPY_MODEL_19421384e1314301bed2f6dba6cba42e",
            "value": " 582/582 [00:00&lt;00:00, 44.5kB/s]"
          }
        },
        "6b6d7c4868474389bb79c3fd87256d93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e27e57c4164872af97f39c351fc98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce788becd95b4815a57e40ec6874158a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc16d09d39de41fab6157b4e254fde0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "508f9d4273ef4cfe8f95b12f5708ddc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c51ae66864d4ccd8a368a9f38f4f468": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19421384e1314301bed2f6dba6cba42e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dfcef4955a94ea0aff24e14c87db175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e7662ea7c3e48c8a5ee1726daa91650",
              "IPY_MODEL_c054c9d6cf7d4614ac52bace7b4ac7d7",
              "IPY_MODEL_dd9507ff06b446ba9c36226607eddb44"
            ],
            "layout": "IPY_MODEL_acb279cf047140d2bc2d527d60044fc8"
          }
        },
        "3e7662ea7c3e48c8a5ee1726daa91650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5add3f49a9294b48b5b420b63e917c41",
            "placeholder": "​",
            "style": "IPY_MODEL_d74aa61789314c0cbe502f8242a30599",
            "value": "100%"
          }
        },
        "c054c9d6cf7d4614ac52bace7b4ac7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c77ea82e010462ea3bee705316fb952",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4b28d808c5a40588b8322eb9edc3c2b",
            "value": 1
          }
        },
        "dd9507ff06b446ba9c36226607eddb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5f70703a7ef43eaa132e6dbd4850bda",
            "placeholder": "​",
            "style": "IPY_MODEL_eb14ecf14c2e4472ad783bf141299b30",
            "value": " 1/1 [00:02&lt;00:00,  2.67s/it]"
          }
        },
        "acb279cf047140d2bc2d527d60044fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5add3f49a9294b48b5b420b63e917c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d74aa61789314c0cbe502f8242a30599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c77ea82e010462ea3bee705316fb952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b28d808c5a40588b8322eb9edc3c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5f70703a7ef43eaa132e6dbd4850bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb14ecf14c2e4472ad783bf141299b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb08627ca7e743a3949d92df28d1b899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b3524633f314610b1ce534481a4b4e2",
              "IPY_MODEL_d656053c023e4f4d9aeb712762afe879",
              "IPY_MODEL_f8fe738b08a34dc8bebe2a887a44b523"
            ],
            "layout": "IPY_MODEL_4687a37ed75743d88436cfcab8ee3f5c"
          }
        },
        "1b3524633f314610b1ce534481a4b4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d283939e136043a2a9e66335e6185333",
            "placeholder": "​",
            "style": "IPY_MODEL_68b16221773e4019bc7cf757944dbf18",
            "value": "tokenizer.json: "
          }
        },
        "d656053c023e4f4d9aeb712762afe879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58fe002ce02e45d7ab6be6dd1e193cf8",
            "max": 34356041,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a364325f3aa44a24a70340e3f60a035b",
            "value": 34356041
          }
        },
        "f8fe738b08a34dc8bebe2a887a44b523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f1e2064c58d47a7b80fdbef46c9029c",
            "placeholder": "​",
            "style": "IPY_MODEL_f441ccb5fe4b4a5f9e6ec2842441cc70",
            "value": " 48.0M/? [00:02&lt;00:00, 23.7MB/s]"
          }
        },
        "4687a37ed75743d88436cfcab8ee3f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d283939e136043a2a9e66335e6185333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68b16221773e4019bc7cf757944dbf18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58fe002ce02e45d7ab6be6dd1e193cf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a364325f3aa44a24a70340e3f60a035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f1e2064c58d47a7b80fdbef46c9029c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f441ccb5fe4b4a5f9e6ec2842441cc70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}